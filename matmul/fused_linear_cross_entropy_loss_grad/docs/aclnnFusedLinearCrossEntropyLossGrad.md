# aclnnFusedLinearCrossEntropyLossGrad

[ğŸ“„ æŸ¥çœ‹æºç ](https://gitcode.com/cann/ops-nn/tree/master/matmul/fused_linear_cross_entropy_loss_grad)

## äº§å“æ”¯æŒæƒ…å†µ

|äº§å“             |  æ˜¯å¦æ”¯æŒ  |
|:-------------------------|:----------:|
|  <term>Ascend 950PR/Ascend 950DT</term>   |     Ã—    |
|  <term>Atlas A3 è®­ç»ƒç³»åˆ—äº§å“/Atlas A3 æ¨ç†ç³»åˆ—äº§å“</term>   |     âˆš    |
|  <term>Atlas A2 è®­ç»ƒç³»åˆ—äº§å“/Atlas A2 æ¨ç†ç³»åˆ—äº§å“</term>     |     âˆš    |
|  <term>Atlas 200I/500 A2 æ¨ç†äº§å“</term>    |     Ã—    |
|  <term>Atlas æ¨ç†ç³»åˆ—äº§å“</term>    |     Ã—    |
|  <term>Atlas è®­ç»ƒç³»åˆ—äº§å“</term>    |     Ã—    |

## åŠŸèƒ½è¯´æ˜

- æ¥å£åŠŸèƒ½ï¼šæœ¬ç®—å­æ˜¯è¯æ±‡è¡¨å¹¶è¡Œåœºæ™¯ä¸‹äº¤å‰ç†µæŸå¤±è®¡ç®—æ¨¡å—ä¸­çš„ä¸€éƒ¨åˆ†ï¼Œè§£å†³è¶…å¤§è§„æ¨¡è¯æ±‡è¡¨ä¸‹çš„æ˜¾å­˜å’Œè®¡ç®—æ•ˆç‡é—®é¢˜ï¼Œå½“å‰éƒ¨åˆ†ä¸ºæ¢¯åº¦è®¡ç®—å®ç°ï¼Œç”¨äºè®¡ç®—å¶å­èŠ‚ç‚¹`input`å’Œ`weight`çš„æ¢¯åº¦ã€‚
  éœ€è¦è·å¾—`aclnnFusedLinearOnlineMaxSum`ã€`aclnnFusedCrossEntropyLossWithMaxSum`çš„ç›¸å…³è¾“å‡ºï¼Œä»¥åŠ`logits`ç›¸å…³çš„å…¨å±€é€šä¿¡ç»“æœä½œä¸ºæœ¬æ¥å£è¾“å…¥ã€‚
- è®¡ç®—å…¬å¼ï¼š

&emsp;&emsp;é«˜æ€§èƒ½æ¨¡å¼ï¼ŒsoftmaxOptionalénullptrï¼š

$$
\text{softmax} \in \mathbb{R}^{BT \times V}
$$

$$
\text{arange\_1d} = [0, 1, \dots, BT-1] \in \mathbb{N}^{BT}
$$

$$
\text{softmax\_update} = \mathbf{1} - \text{target\_mask}.view(-1) \in \mathbb{R}^{BT}
$$

$$
\text{softmax}[\text{arange\_1d}, \text{masked\_target}] \leftarrow \text{softmax}[\text{arange\_1d}, \text{masked\_target}] - \text{softmax\_update}
$$

$$
\text{softmax} \leftarrow \text{softmax} \odot \text{grad}.unsqueeze(-1) \in \mathbb{R}^{BT \times V}
$$

$$
\text{grad\_input} = \text{softmax} \cdot \text{weight}^T \in \mathbb{R}^{BT \times H}
$$

$$
\text{grad\_weight} = \text{softmax}^T \cdot \text{input} \in \mathbb{R}^{V \times H}
$$

</br>
&emsp;&emsp;çœæ˜¾å­˜æ¨¡å¼ï¼ŒsoftmaxOptionalä¸ºnullptrï¼š

$$
\text{vocab\_parallel\_logits} = \text{input} \cdot \text{weight}^T \quad \in \mathbb{R}^{BT \times V}
$$

$$
\text{logits\_sub} = \text{vocab\_parallel\_logits} - \text{logits\_max}.unsqueeze(-1) \quad \in \mathbb{R}^{BT \times V}
$$

$$
\text{exp\_logits} = \exp(\text{logits\_sub}) \quad \in \mathbb{R}^{BT \times V}
$$

$$
\text{exp\_logits} \gets \frac{\text{exp\_logits}}{\text{sum\_exp\_logits}.unsqueeze(-1)} \quad \in \mathbb{R}^{BT \times V}
$$

$$
\text{grad\_logits} = \text{exp\_logits} \quad \in \mathbb{R}^{BT \times V}
$$

$$
\text{grad\_2d} = \text{grad\_logits}.view(-1, \text{partition\_vocab\_size}) \quad \in \mathbb{R}^{BT \times V}
$$

$$
\text{arange\_1d} = [0, 1, \dots, BT-1] \quad \in \mathbb{N}^{BT}
$$

$$
\text{softmax\_update} = 1 - \text{target\_mask}.view(-1) \quad \in \mathbb{R}^{BT}
$$

$$
\text{grad\_2d}[\text{arange\_1d}, \text{masked\_target\_1d}] \gets \text{grad\_2d}[\text{arange\_1d}, \text{masked\_target\_1d}] - \text{softmax\_update}
$$

$$
\text{grad\_logits} \gets \text{grad\_logits} \odot \text{grad}.unsqueeze(-1) \quad \in \mathbb{R}^{BT \times V}
$$

$$
\text{grad\_input} = \text{grad\_logits} \cdot \text{weight} \quad \in \mathbb{R}^{BT \times H}
$$

$$
\text{grad\_weight} = \text{grad\_logits}^T \cdot \text{input} \quad \in \mathbb{R}^{V \times H}
$$

## å‡½æ•°åŸå‹

æ¯ä¸ªç®—å­åˆ†ä¸º[ä¸¤æ®µå¼æ¥å£](../../../docs/zh/context/ä¸¤æ®µå¼æ¥å£.md)ï¼Œå¿…é¡»å…ˆè°ƒç”¨`aclnnFusedLinearCrossEntropyLossGradGetWorkspaceSize`æ¥å£è·å–è®¡ç®—æ‰€éœ€workspaceå¤§å°ä»¥åŠåŒ…å«äº†ç®—å­è®¡ç®—æµç¨‹çš„æ‰§è¡Œå™¨ï¼Œå†è°ƒç”¨`aclnnFusedLinearCrossEntropyLossGrad`æ¥å£æ‰§è¡Œè®¡ç®—ã€‚

  ```Cpp
  aclnnStatus aclnnFusedLinearCrossEntropyLossGradGetWorkspaceSize(
    const aclTensor   *grad,
    const aclTensor   *input,
    const aclTensor   *weight,
    const aclTensor   *targetMask,
    const aclTensor   *maskedTarget,
    float              labelSmoothing,
    const aclTensor   *logitsMaxOptional,
    const aclTensor   *sumExpLogitsOptional,
    const aclTensor   *softmaxOptional,
    aclTensor         *inputGradOut,
    aclTensor         *weightGradOut,
    uint64_t          *workspaceSize,
    aclOpExecutor    **executor)
  ```
  ```Cpp
  aclnnStatus aclnnFusedLinearCrossEntropyLossGrad(
    void             *workspace,
    uint64_t          workspaceSize,
    aclOpExecutor    *executor,
    aclrtStream       stream)
  ```

## aclnnFusedLinearCrossEntropyLossGradGetWorkspaceSize

- **å‚æ•°è¯´æ˜**

  <table style="undefined;table-layout: fixed; width: 1478px"><colgroup>
    <col style="width: 169px">
    <col style="width: 121px">
    <col style="width: 264px">
    <col style="width: 253px">
    <col style="width: 242px">
    <col style="width: 148px">
    <col style="width: 135px">
    <col style="width: 146px">
    </colgroup>
    <thead>
      <tr>
        <th>å‚æ•°å</th>
        <th>è¾“å…¥/è¾“å‡º</th>
        <th>æè¿°</th>
        <th>ä½¿ç”¨è¯´æ˜</th>
        <th>æ•°æ®ç±»å‹</th>
        <th>æ•°æ®æ ¼å¼</th>
        <th>ç»´åº¦(shape)</th>
        <th>éè¿ç»­Tensor</th>
      </tr></thead>
    <tbody>
      <tr>
        <td>grad</td>
        <td>è¾“å…¥</td>
        <td>å½“å‰èŠ‚ç‚¹çš„æ¢¯åº¦ï¼Œå…¬å¼ä¸­çš„è¾“å…¥gradã€‚</td>
        <td>æ”¯æŒç©ºtensorã€‚</td>
        <td>FLOAT32</td>
        <td>ND</td>
        <td>1</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>input</td>
        <td>è¾“å…¥</td>
        <td>çŸ©é˜µä¹˜çš„è¾“å…¥çŸ©é˜µï¼Œå…¬å¼ä¸­çš„è¾“å…¥inputã€‚</td>
        <td>ç¬¬1ç»´é•¿åº¦ä¸è¾“å…¥gradçš„é•¿åº¦ç›¸åŒï¼Œæ”¯æŒç©ºtensorã€‚</td>
        <td>FLOAT16ã€BFLOAT16</td>
        <td>ND</td>
        <td>2</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>weight</td>
        <td>è¾“å…¥</td>
        <td>çŸ©é˜µä¹˜çš„æƒé‡çŸ©é˜µï¼Œå…¬å¼ä¸­çš„weightã€‚</td>
        <td>æ•°æ®ç±»å‹ä¸è¾“å…¥inputç›¸åŒï¼Œshapeç¬¬1ç»´é•¿åº¦ä¸æ”¯æŒå°äº128ï¼Œç¬¬2ç»´é•¿åº¦ä¸è¾“å…¥inputçš„ç¬¬2ç»´é•¿åº¦ç›¸åŒã€‚</td>
        <td>FLOAT16ã€BFLOAT16</td>
        <td>ND</td>
        <td>2</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>targetMask</td>
        <td>è¾“å…¥</td>
        <td>ä¸­é—´å˜é‡ï¼Œä»£è¡¨å¯¹åº”è¯IDæ˜¯å¦åœ¨ç›®æ ‡èŒƒå›´å†…ï¼Œå…¬å¼ä¸­çš„target_maskã€‚</td>
        <td><ul><li>æ¯1bitæ•°æ®ä»£è¡¨1ä¸ªå¸ƒå°”å€¼ï¼Œ0ä»£è¡¨falseï¼Œ1ä»£è¡¨trueã€‚</li><li>shapeé•¿åº¦ä¹˜ä»¥8é¡»ä¸å°äºè¾“å…¥gradçš„é•¿åº¦ã€‚</li></ul></td>
        <td>UINT8</td>
        <td>ND</td>
        <td>1</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>maskedTarget</td>
        <td>è¾“å…¥</td>
        <td>ä¸­é—´å˜é‡ï¼Œä»£è¡¨å¯¹åº”è¯IDæ˜ å°„åˆ°å½“å‰è®¾å¤‡è¯æ±‡è¡¨åˆ†ç‰‡çš„å±€éƒ¨ç´¢å¼•ï¼Œæ— æ•ˆç›®æ ‡ä¼šè¢«æ©ç targetMaskå¤„ç†ï¼Œå…¬å¼ä¸­çš„masked_targetã€‚</td>
        <td>shapeé•¿åº¦ä¸è¾“å…¥gradç›¸åŒã€‚</td>
        <td>INT64ã€INT32</td>
        <td>ND</td>
        <td>1</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>labelSmoothing</td>
        <td>è¾“å…¥</td>
        <td>æ ‡ç­¾å¹³æ»‘ç³»æ•°ï¼Œç”¨äºç¼“è§£è¿‡æ‹Ÿåˆã€‚</td>
        <td>ç›®å‰ä»…æ”¯æŒå–å€¼ä¸º0ã€‚</td>
        <td>FLOAT32</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>logitsMaxOptional</td>
        <td>å¯é€‰è¾“å…¥</td>
        <td>ä¸­é—´å˜é‡ï¼Œå…¨å±€logitsçš„æœ€å¤§å€¼ï¼Œå…¬å¼ä¸­çš„logits_maxã€‚</td>
        <td><ul><li>æ”¯æŒè¾“å…¥nullptrï¼Œè¾“å…¥nullptrçš„åœºæ™¯éœ€è¦æä¾›æœ‰æ•ˆçš„softmaxOptionalè¾“å…¥ã€‚</li><li>shapeé•¿åº¦ä¸è¾“å…¥gradç›¸åŒã€‚</li></ul></td>
        <td>FLOAT32</td>
        <td>ND</td>
        <td>1</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>sumExpLogitsOptional</td>
        <td>å¯é€‰è¾“å…¥</td>
        <td>ä¸­é—´å˜é‡ï¼Œå¤„ç†åçš„logitsï¼Œå…¬å¼ä¸­çš„sum_exp_logitsã€‚</td>
        <td><ul><li>æ”¯æŒè¾“å…¥nullptrï¼Œè¾“å…¥nullptrçš„åœºæ™¯éœ€è¦æä¾›æœ‰æ•ˆçš„softmaxOptionalè¾“å…¥ã€‚</li><li>shapeé•¿åº¦ä¸è¾“å…¥gradç›¸åŒã€‚</li></ul></td>
        <td>FLOAT32</td>
        <td>ND</td>
        <td>1</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>softmaxOptional</td>
        <td>å¯é€‰è¾“å…¥</td>
        <td>ä¸­é—´å˜é‡ï¼ŒçŸ©é˜µä¹˜çš„ç»“æœï¼Œå…¬å¼ä¸­çš„softmaxã€‚</td>
        <td><ul><li>æ”¯æŒè¾“å…¥nullptrï¼Œè¾“å…¥nullptræ—¶é¡»æä¾›æœ‰æ•ˆçš„logitsMaxOptionalã€sumExpLogitsOptionalè¾“å…¥ï¼›è¾“å…¥énullptræ—¶ï¼ŒlogitsMaxOptionalã€sumExpLogitsOptionalè¾“å…¥æ— æ•ˆã€‚</li><li>shapeç¬¬1ç»´é•¿åº¦ä¸è¾“å…¥gradé•¿åº¦ç›¸åŒï¼Œç¬¬äºŒç»´é•¿åº¦ä¸è¾“å…¥weightçš„ç¬¬1ç»´é•¿åº¦ç›¸åŒã€‚</li></ul></td>
        <td>FLOAT32</td>
        <td>ND</td>
        <td>2</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>inputGradOut</td>
        <td>è¾“å‡º</td>
        <td>å¯¹åº”å¶å­èŠ‚ç‚¹inputçš„æ¢¯åº¦ï¼Œå…¬å¼ä¸­çš„grad_inputã€‚</td>
        <td><ul><li>æ•°æ®ç±»å‹ä¸è¾“å…¥inputç›¸åŒã€‚</li><li>shapeç¬¬1ç»´é•¿åº¦ä¸è¾“å…¥gradé•¿åº¦ç›¸åŒï¼Œç¬¬2ç»´é•¿åº¦ä¸è¾“å…¥weightçš„ç¬¬2ç»´é•¿åº¦ç›¸åŒã€‚</li></ul></td>
        <td>FLOAT16ã€BFLOAT16</td>
        <td>ND</td>
        <td>2</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>weightGradOut</td>
        <td>è¾“å‡º</td>
        <td>å¯¹åº”å¶å­èŠ‚ç‚¹weightçš„æ¢¯åº¦ï¼Œå…¬å¼ä¸­çš„grad_weightã€‚</td>
        <td><ul><li>æ•°æ®ç±»å‹ä¸è¾“å…¥inputç›¸åŒã€‚</li><li>shapeç¬¬1ç»´é•¿åº¦ä¸è¾“å…¥weightçš„ç¬¬1ç»´é•¿åº¦ç›¸åŒï¼Œç¬¬2ç»´é•¿åº¦ä¸è¾“å…¥gradé•¿åº¦ç›¸åŒã€‚</li></ul></td>
        <td>FLOAT16ã€BFLOAT16</td>
        <td>ND</td>
        <td>2</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>workspaceSize</td>
        <td>è¾“å‡º</td>
        <td>è¿”å›éœ€è¦åœ¨Deviceä¾§ç”³è¯·çš„workspaceå¤§å°ã€‚</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>executor</td>
        <td>è¾“å‡º</td>
        <td>è¿”å›opæ‰§è¡Œå™¨ï¼ŒåŒ…å«äº†ç®—å­è®¡ç®—æµç¨‹ã€‚</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
    </tbody></table>

- **è¿”å›å€¼**
  
  aclnnStatusï¼šè¿”å›çŠ¶æ€ç ï¼Œå…·ä½“å‚è§[aclnnè¿”å›ç ](../../../docs/zh/context/aclnnè¿”å›ç .md)ã€‚

  ç¬¬ä¸€æ®µæ¥å£å®Œæˆå…¥å‚æ ¡éªŒï¼Œå‡ºç°ä»¥ä¸‹åœºæ™¯æ—¶æŠ¥é”™ï¼š

  <table style="undefined;table-layout: fixed; width: 1149px"><colgroup>
  <col style="width: 281px">
  <col style="width: 119px">
  <col style="width: 749px">
  </colgroup>
  <thead>
    <tr>
      <th>è¿”å›ç </th>
      <th>é”™è¯¯ç </th>
      <th>æè¿°</th>
    </tr></thead>
  <tbody>
    <tr>
      <td rowspan="2">ACLNN_ERR_PARAM_NULLPTR</td>
      <td rowspan="2">161001</td>
      <td>ä¼ å…¥çš„éå¯é€‰è¾“å…¥æ˜¯ç©ºæŒ‡é’ˆã€‚</td>
    </tr>
    <tr>
      <td>ä¼ å…¥çš„softmaxOptionalä¸ºç©ºæŒ‡é’ˆçš„åœºæ™¯ä¸‹ï¼ŒlogitsMaxOptionalæˆ–sumExpLogitsOptionalä¸ºç©ºæŒ‡é’ˆã€‚</td>
    </tr>
    <tr>
      <td rowspan="4">ACLNN_ERR_PARAM_INVALID</td>
      <td rowspan="4">161002</td>
      <td>è¾“å…¥çš„labelSmoothingå–å€¼ä¸æ”¯æŒã€‚</td>
    </tr>
    <tr>
      <td>è¾“å…¥çš„æ•°æ®ç±»å‹ä¸åœ¨æ”¯æŒçš„èŒƒå›´å†…ã€‚</td>
    </tr>
    <tr>
      <td>è¾“å…¥çš„æ•°æ®æ ¼å¼ä¸åœ¨æ”¯æŒçš„èŒƒå›´å†…ã€‚</td>
    </tr>
    <tr>
      <td>è¾“å…¥çš„shapeä¸åœ¨æ”¯æŒçš„èŒƒå›´å†…ï¼Œä¸æ»¡è¶³é•¿åº¦è¦æ±‚ã€‚</td>
    </tr>
    <tr>
      <td>ACLNN_ERR_RUNTIME_ERROR</td>
      <td>361001</td>
      <td>å½“å‰å¹³å°ä¸åœ¨æ”¯æŒçš„å¹³å°èŒƒå›´å†…ã€‚</td>
    </tr>
  </tbody>
  </table>

## aclnnFusedLinearCrossEntropyLossGrad

- **å‚æ•°è¯´æ˜**

  <table style="undefined;table-layout: fixed; width: 1150px"><colgroup>
  <col style="width: 168px">
  <col style="width: 128px">
  <col style="width: 854px">
  </colgroup>
  <thead>
    <tr>
      <th>å‚æ•°å</th>
      <th>è¾“å…¥/è¾“å‡º</th>
      <th>æè¿°</th>
    </tr></thead>
  <tbody>
    <tr>
      <td>workspace</td>
      <td>è¾“å…¥</td>
      <td>åœ¨Deviceä¾§ç”³è¯·çš„workspaceå†…å­˜åœ°å€ã€‚</td>
    </tr>
    <tr>
      <td>workspaceSize</td>
      <td>è¾“å…¥</td>
      <td>åœ¨Deviceä¾§ç”³è¯·çš„workspaceå¤§å°ï¼Œç”±ç¬¬ä¸€æ®µæ¥å£aclnnFusedLinearCrossEntropyLossGradGetWorkspaceSizeè·å–ã€‚</td>
    </tr>
    <tr>
      <td>executor</td>
      <td>è¾“å…¥</td>
      <td>opæ‰§è¡Œå™¨ï¼ŒåŒ…å«äº†ç®—å­è®¡ç®—æµç¨‹ã€‚</td>
    </tr>
    <tr>
      <td>stream</td>
      <td>è¾“å…¥</td>
      <td>æŒ‡å®šæ‰§è¡Œä»»åŠ¡çš„Streamã€‚</td>
    </tr>
  </tbody>
  </table>
  
- **è¿”å›å€¼**
  
  aclnnStatusï¼šè¿”å›çŠ¶æ€ç ï¼Œå…·ä½“å‚è§[aclnnè¿”å›ç ](../../../docs/zh/context/aclnnè¿”å›ç .md)ã€‚

## çº¦æŸè¯´æ˜
- ç¡®å®šæ€§è¯´æ˜ï¼š
  - aclnnFusedLinearCrossEntropyLossGradé»˜è®¤ç¡®å®šæ€§å®ç°ã€‚

## è°ƒç”¨ç¤ºä¾‹

ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼Œä»…ä¾›å‚è€ƒï¼Œå…·ä½“ç¼–è¯‘å’Œæ‰§è¡Œè¿‡ç¨‹è¯·å‚è€ƒ[ç¼–è¯‘ä¸è¿è¡Œæ ·ä¾‹](../../../docs/zh/context/ç¼–è¯‘ä¸è¿è¡Œæ ·ä¾‹.md)ã€‚

```c++
#include <iostream>
#include <vector>
#include "acl/acl.h"
#include "aclnnop/aclnn_fused_linear_cross_entropy_loss_grad.h"

#define CHECK_RET(cond, return_expr) \
    do                               \
    {                                \
        if (!(cond))                 \
        {                            \
            return_expr;             \
        }                            \
    } while (0)

#define LOG_PRINT(message, ...)         \
    do                                  \
    {                                   \
        printf(message, ##__VA_ARGS__); \
    } while (0)

int64_t GetShapeSize(const std::vector<int64_t> &shape)
{
    int64_t shapeSize = 1;
    for (auto i : shape) {
        shapeSize *= i;
    }
    return shapeSize;
}

int Init(int32_t deviceId, aclrtStream *stream)
{
    // å›ºå®šå†™æ³•ï¼Œèµ„æºåˆå§‹åŒ–
    auto ret = aclInit(nullptr);
    CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclInit failed. ERROR: %d\n", ret); return ret);
    ret = aclrtSetDevice(deviceId);
    CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtSetDevice failed. ERROR: %d\n", ret); return ret);
    ret = aclrtCreateStream(stream);
    CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtCreateStream failed. ERROR: %d\n", ret); return ret);
    return 0;
}

template <typename T>
std::vector<T> GenZeroVector(const std::vector<int64_t>& shape) {
    // 1. è®¡ç®—æ€»å…ƒç´ æ•°é‡
    size_t total = 1;
    for (auto dim : shape) {
        total *= dim;
    }

    // 2. å¡«å……0
    std::vector<T> vec(total);
    for (auto& elem : vec) {
        elem = 0;
    }
    return vec;
}

template <typename T>
int CreateAclTensor(const std::vector<T> &hostData, const std::vector<int64_t> &shape, void **deviceAddr,
                    aclDataType dataType, aclTensor **tensor)
{
    auto size = GetShapeSize(shape) * sizeof(T);
    // è°ƒç”¨aclrtMallocç”³è¯·deviceä¾§å†…å­˜
    auto ret = aclrtMalloc(deviceAddr, size, ACL_MEM_MALLOC_HUGE_FIRST);
    CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtMalloc failed. ERROR: %d\n", ret); return ret);
    // è°ƒç”¨aclrtMemcpyå°†hostä¾§æ•°æ®æ‹·è´åˆ°deviceä¾§å†…å­˜ä¸Š
    ret = aclrtMemcpy(*deviceAddr, size, hostData.data(), size, ACL_MEMCPY_HOST_TO_DEVICE);
    CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtMemcpy failed. ERROR: %d\n", ret); return ret);

    // è®¡ç®—è¿ç»­tensorçš„strides
    std::vector<int64_t> strides(shape.size(), 1);
    for (int64_t i = shape.size() - 2; i >= 0; i--) {
        strides[i] = shape[i + 1] * strides[i + 1];
    }

    // è°ƒç”¨aclCreateTensoræ¥å£åˆ›å»ºaclTensor
    *tensor = aclCreateTensor(shape.data(), shape.size(), dataType, strides.data(), 0, aclFormat::ACL_FORMAT_ND,
                              shape.data(), shape.size(), *deviceAddr);
    return 0;
}

template <typename T>
int CreateEmptyAclTensor(const std::vector<int64_t> &shape, void **deviceAddr,
                         aclDataType dataType, aclTensor **tensor)
{
    auto size = GetShapeSize(shape) * sizeof(T);
    // è°ƒç”¨aclrtMallocç”³è¯·deviceä¾§å†…å­˜
    auto ret = aclrtMalloc(deviceAddr, size, ACL_MEM_MALLOC_HUGE_FIRST);
    CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtMalloc failed. ERROR: %d\n", ret); return ret);

    // è®¡ç®—è¿ç»­tensorçš„strides
    std::vector<int64_t> strides(shape.size(), 1);
    for (int64_t i = shape.size() - 2; i >= 0; i--) {
        strides[i] = shape[i + 1] * strides[i + 1];
    }

    // è°ƒç”¨aclCreateTensoræ¥å£åˆ›å»ºaclTensor
    *tensor = aclCreateTensor(shape.data(), shape.size(), dataType, strides.data(), 0, aclFormat::ACL_FORMAT_ND,
                              shape.data(), shape.size(), *deviceAddr);
    return 0;
}

int main()
{
    // 1. ï¼ˆå›ºå®šå†™æ³•ï¼‰device/streamåˆå§‹åŒ–ï¼Œå‚è€ƒaclå¯¹å¤–æ¥å£åˆ—è¡¨
    // æ ¹æ®è‡ªå·±çš„å®é™…deviceå¡«å†™deviceId
    int32_t deviceId = 0;
    aclrtStream stream;
    auto ret = Init(deviceId, &stream);
    CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("Init acl failed. ERROR: %d\n", ret); return ret);

    // 2. æ„é€ è¾“å…¥ä¸è¾“å‡ºï¼Œéœ€è¦æ ¹æ®APIçš„æ¥å£è‡ªå®šä¹‰æ„é€ 
    int64_t BT = 1024;
    int64_t V = 1024;
    int64_t H = 1024;
    std::vector<int64_t> gradShape = {BT};
    std::vector<int64_t> inputShape = {BT, H};
    std::vector<int64_t> weightShape = {V, H};
    std::vector<int64_t> targetMaskShape = {BT};
    std::vector<int64_t> maskedTargetShape = {BT};
    std::vector<int64_t> softmaxOptionalShape = {BT, V};
    std::vector<int64_t> inputGradOutShape = {BT, H};
    std::vector<int64_t> weightGradOutShape = {V, H};
    void *gradDeviceAddr = nullptr;
    void *inputDeviceAddr = nullptr;
    void *weightDeviceAddr = nullptr;
    void *targetMaskDeviceAddr = nullptr;
    void *maskedTargetDeviceAddr = nullptr;
    void *softmaxOptionalDeviceAddr = nullptr;
    void *inputGradOutDeviceAddr = nullptr;
    void *weightGradOutDeviceAddr = nullptr;
    aclTensor *grad = nullptr;
    aclTensor *input = nullptr;
    aclTensor *weight = nullptr;
    aclTensor *targetMask = nullptr;
    aclTensor *maskedTarget = nullptr;
    float labelSmoothing = 0.0;
    aclTensor *logitsMaxOptional = nullptr;
    aclTensor *sumExpLogitsOptional = nullptr;
    aclTensor *softmaxOptional = nullptr;
    aclTensor *inputGradOut = nullptr;
    aclTensor *weightGradOut = nullptr;
    // åˆ›å»ºaclTensor
    auto gradData = GenZeroVector<int32_t>(gradShape);
    ret = CreateAclTensor<int32_t>(gradData, gradShape, &gradDeviceAddr, aclDataType::ACL_FLOAT, &grad);
    CHECK_RET(ret == ACL_SUCCESS, return ret);
    auto inputData = GenZeroVector<int16_t>(inputShape);
    ret = CreateAclTensor<int16_t>(inputData, inputShape, &inputDeviceAddr, aclDataType::ACL_BF16, &input);
    CHECK_RET(ret == ACL_SUCCESS, return ret);
    auto weightData = GenZeroVector<int16_t>(weightShape);
    ret = CreateAclTensor<int16_t>(weightData, weightShape, &weightDeviceAddr, aclDataType::ACL_BF16, &weight);
    CHECK_RET(ret == ACL_SUCCESS, return ret);
    auto targetMaskData = GenZeroVector<int8_t>(targetMaskShape);
    ret = CreateAclTensor<int8_t>(targetMaskData, targetMaskShape, &targetMaskDeviceAddr, aclDataType::ACL_UINT8, &targetMask);
    CHECK_RET(ret == ACL_SUCCESS, return ret);
    auto maskedTargetData = GenZeroVector<int32_t>(maskedTargetShape);
    ret = CreateAclTensor<int32_t>(maskedTargetData, maskedTargetShape, &maskedTargetDeviceAddr, aclDataType::ACL_INT32, &maskedTarget);
    CHECK_RET(ret == ACL_SUCCESS, return ret);
    auto softmaxOptionalData = GenZeroVector<int32_t>(softmaxOptionalShape);
    ret = CreateAclTensor<int32_t>(softmaxOptionalData, softmaxOptionalShape, &softmaxOptionalDeviceAddr, aclDataType::ACL_FLOAT, &softmaxOptional);
    CHECK_RET(ret == ACL_SUCCESS, return ret);
    auto inputGradOutData = GenZeroVector<int16_t>(inputGradOutShape);
    ret = CreateAclTensor<int16_t>(inputGradOutData, inputGradOutShape, &inputGradOutDeviceAddr, aclDataType::ACL_BF16, &inputGradOut);
    CHECK_RET(ret == ACL_SUCCESS, return ret);
    auto weightGradOutData = GenZeroVector<int16_t>(weightGradOutShape);
    ret = CreateAclTensor<int16_t>(weightGradOutData, weightGradOutShape, &weightGradOutDeviceAddr, aclDataType::ACL_BF16, &weightGradOut);
    CHECK_RET(ret == ACL_SUCCESS, return ret);

    // 3. è°ƒç”¨CANNç®—å­åº“APIï¼Œéœ€è¦ä¿®æ”¹ä¸ºå…·ä½“çš„Apiåç§°
    uint64_t workspaceSize = 0;
    aclOpExecutor *executor;
    // è°ƒç”¨aclnnFusedLinearCrossEntropyLossGradç¬¬ä¸€æ®µæ¥å£
    ret = aclnnFusedLinearCrossEntropyLossGradGetWorkspaceSize(grad, input, weight, targetMask, maskedTarget, labelSmoothing, logitsMaxOptional, sumExpLogitsOptional, softmaxOptional, inputGradOut, weightGradOut, &workspaceSize, &executor);
    CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclnnFusedLinearCrossEntropyLossGradGetWorkspaceSize failed. ERROR: %d\n", ret); return ret);
    // æ ¹æ®ç¬¬ä¸€æ®µæ¥å£è®¡ç®—å‡ºçš„workspaceSizeç”³è¯·deviceå†…å­˜
    void *workspaceAddr = nullptr;
    if (workspaceSize > 0) {
        ret = aclrtMalloc(&workspaceAddr, workspaceSize, ACL_MEM_MALLOC_HUGE_FIRST);
        CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("allocate workspace failed. ERROR: %d\n", ret); return ret);
    }
    // è°ƒç”¨aclnnFusedLinearCrossEntropyLossGradç¬¬äºŒæ®µæ¥å£
    ret = aclnnFusedLinearCrossEntropyLossGrad(workspaceAddr, workspaceSize, executor, stream);
    CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclnnFusedLinearCrossEntropyLossGrad failed. ERROR: %d\n", ret); return ret);

    // 4. ï¼ˆå›ºå®šå†™æ³•ï¼‰åŒæ­¥ç­‰å¾…ä»»åŠ¡æ‰§è¡Œç»“æŸ
    ret = aclrtSynchronizeStream(stream);
    CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtSynchronizeStream failed. ERROR: %d\n", ret); return ret);

    // 5. è·å–è¾“å‡ºçš„å€¼ï¼Œå°†deviceä¾§å†…å­˜ä¸Šçš„ç»“æœæ‹·è´è‡³hostä¾§ï¼Œéœ€è¦æ ¹æ®å…·ä½“APIçš„æ¥å£å®šä¹‰ä¿®æ”¹
    // inputGradOut
    auto size = GetShapeSize(inputGradOutShape);
    std::vector<float> resultData(size, 0);
    ret = aclrtMemcpy(resultData.data(), resultData.size() * sizeof(resultData[0]), inputGradOutDeviceAddr, size * sizeof(resultData[0]), ACL_MEMCPY_DEVICE_TO_HOST);
    CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("copy result from device to host failed. ERROR: %d\n", ret); return ret);
    for (int64_t i = 0; i < 16; i++) {
        LOG_PRINT("result[%ld] is: %f\n", i, resultData[i]);
    }

    // 6. é‡Šæ”¾aclTensorï¼Œéœ€è¦æ ¹æ®å…·ä½“APIçš„æ¥å£å®šä¹‰ä¿®æ”¹
    aclDestroyTensor(grad);
    aclDestroyTensor(input);
    aclDestroyTensor(weight);
    aclDestroyTensor(targetMask);
    aclDestroyTensor(maskedTarget);
    aclDestroyTensor(softmaxOptional);
    aclDestroyTensor(inputGradOut);
    aclDestroyTensor(weightGradOut);

    // 7. é‡Šæ”¾Deviceèµ„æºï¼Œéœ€è¦æ ¹æ®å…·ä½“APIçš„æ¥å£å®šä¹‰ä¿®æ”¹
    aclrtFree(gradDeviceAddr);
    aclrtFree(inputDeviceAddr);
    aclrtFree(weightDeviceAddr);
    aclrtFree(targetMaskDeviceAddr);
    aclrtFree(maskedTargetDeviceAddr);
    aclrtFree(softmaxOptionalDeviceAddr);
    aclrtFree(inputGradOutDeviceAddr);
    aclrtFree(weightGradOutDeviceAddr);
    if (workspaceSize > 0) {
        aclrtFree(workspaceAddr);
    }
    aclrtDestroyStream(stream);
    aclrtResetDevice(deviceId);
    aclFinalize();

    return 0;
}
```
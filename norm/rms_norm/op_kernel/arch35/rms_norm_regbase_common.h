/**
 * Copyright (c) 2025 Huawei Technologies Co., Ltd.
 * This program is free software, you can redistribute it and/or modify it under the terms and conditions of
 * CANN Open Software License Agreement Version 2.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */

/*!
 * \file rms_norm_regbase_common.h
 * \brief RmsNorm regbase common
 */
#ifndef OPS_BUILT_IN_TBE_IMPL_ASCENDC_RMS_NORM_REGBASE_COMMON_H
#define OPS_BUILT_IN_TBE_IMPL_ASCENDC_RMS_NORM_REGBASE_COMMON_H
#include "kernel_operator.h"
#include "kernel_tiling/kernel_tiling.h"
#include "kernel_utils.h"
#include "../rms_norm_base.h"
#include "../../norm_common/reduce_common_regbase.h"

namespace RmsNorm {
using namespace AscendC;
using namespace AscendC::MicroAPI;
using AscendC::MicroAPI::CreateMask;
using AscendC::MicroAPI::LoadDist;
using AscendC::MicroAPI::LocalMemBar;
using AscendC::MicroAPI::MaskPattern;
using AscendC::MicroAPI::MaskReg;
using AscendC::MicroAPI::MemType;
using AscendC::MicroAPI::RegTensor;
using AscendC::MicroAPI::StoreDist;
using AscendC::MicroAPI::UpdateMask;
using namespace NormCommon;
using namespace NormCommon::NormCommonRegbase;

/*!
 * DataCopy custom implement
 * @tparam T DataCopy data type
 * @tparam U Destination Operand type
 * @tparam R Source Operand type
 * @param dstTensor Destination Tensor
 * @param srcTensor Source Tensor
 * @param blockCount burst
 * @param blockLen burst length
 * @param padParams pad params
 * @return void
 */
template <typename T, typename U, typename R>
__aicore__ inline void DataCopyImpl(
    const U& dstTensor, const R& srcTensor, uint32_t blockCount, uint32_t blockLen, uint32_t srcStride = 0,
    uint32_t dstStride = 0, const DataCopyPadExtParams<T> padParams = {false, 0, 0, 0})
{
    DataCopyExtParams extParams{
        static_cast<uint16_t>(blockCount),           // blockCount
        static_cast<uint32_t>(blockLen * sizeof(T)), // blockLen
        srcStride,                                   // srcStride
        dstStride,                                   // dstStride
        0                                            // rsv
    };
    if constexpr (is_same<U, AscendC::LocalTensor<T>>::value) {
        DataCopyPad(dstTensor, srcTensor, extParams, padParams);
    } else {
        DataCopyPad(dstTensor, srcTensor, extParams);
    }
}

/*!
 * x = x * x + scalar
 * @param dstLocal dst Tensor
 * @param srcLocal src Tensor
 * @param scalar scalar
 * @param count vector size
 * @return
 */
__aicore__ inline void ComputeFormer(
    LocalTensor<float>& dstLocal, LocalTensor<float>& srcLocal, float scalar, uint32_t count)
{
    __local_mem__ float* srcAddr = (__ubuf__ float*)srcLocal.GetPhyAddr();
    __local_mem__ float* dstAddr = (__ubuf__ float*)dstLocal.GetPhyAddr();

    uint32_t calCount = count;
    uint32_t sreg = (uint32_t)calCount;
    uint16_t repeatTimes = CeilDivision(calCount, V_LENGTH);
    __VEC_SCOPE__
    {
        RegTensor<float> vReg, vRegTmp;
        MaskReg maskReg;
        for (uint16_t i = 0; i < (uint16_t)repeatTimes; i++) {
            maskReg = UpdateMask<float>(sreg);
            DataCopy(vReg, srcAddr + i * V_LENGTH);
            Mul(vRegTmp, vReg, vReg, maskReg);
            Muls(vReg, vRegTmp, scalar, maskReg);
            DataCopy(dstAddr + i * V_LENGTH, vReg, maskReg);
        }
    }
}

/*!
 * x = x * x * scalar
 * @param dstLocal dst Tensor
 * @param xFp32 src.as(float32) Tensor
 * @param srcLocal src Tensor
 * @param scalar scalar
 * @param count vector size
 * @return
 */
template <typename T>
__aicore__ inline void ComputeFormer(
    LocalTensor<float>& dstLocal, LocalTensor<float>& xFp32, LocalTensor<T>& srcLocal, float scalar, uint32_t count)
{
    uint32_t calCount = count / 2;
    uint32_t sreg = (uint32_t)calCount;
    uint16_t repeatTimes = CeilDivision(calCount, V_LENGTH);

    __local_mem__ T* xAddr1 = (__ubuf__ T*)srcLocal.GetPhyAddr();
    __local_mem__ T* xAddr2 = (__ubuf__ T*)srcLocal.GetPhyAddr() + calCount;
    __local_mem__ float* dstAddr1 = (__ubuf__ float*)dstLocal.GetPhyAddr();
    __local_mem__ float* dstAddr2 = (__ubuf__ float*)dstLocal.GetPhyAddr() + calCount;
    __local_mem__ float* xFp32Addr1 = (__ubuf__ float*)xFp32.GetPhyAddr();
    __local_mem__ float* xFp32Addr2 = (__ubuf__ float*)xFp32.GetPhyAddr() + calCount;

    if constexpr (IsSameType<T, half>::value) {
        __VEC_SCOPE__
        {
            RegTensor<half> xFp16Reg1, xFp16Reg2;
            RegTensor<float> xFp32Reg1, vReg1, vRegTmp1, xFp32Reg2, vReg2, vRegTmp2;
            MaskReg maskReg;
            for (uint16_t i = 0; i < (uint16_t)repeatTimes; i++) {
                maskReg = UpdateMask<float>(sreg);
                DataCopy<half, LoadDist::DIST_UNPACK_B16>(xFp16Reg1, xAddr1 + i * V_LENGTH);
                DataCopy<half, LoadDist::DIST_UNPACK_B16>(xFp16Reg2, xAddr2 + i * V_LENGTH);
                Cast<float, half, castTraitB162B32>(xFp32Reg1, xFp16Reg1, maskReg);
                Cast<float, half, castTraitB162B32>(xFp32Reg2, xFp16Reg2, maskReg);
                Mul(vRegTmp1, xFp32Reg1, xFp32Reg1, maskReg);
                Mul(vRegTmp2, xFp32Reg2, xFp32Reg2, maskReg);
                Muls(vReg1, vRegTmp1, scalar, maskReg);
                Muls(vReg2, vRegTmp2, scalar, maskReg);
                DataCopy(dstAddr1 + i * V_LENGTH, vReg1, maskReg);
                DataCopy(dstAddr2 + i * V_LENGTH, vReg2, maskReg);
                DataCopy(xFp32Addr1 + i * V_LENGTH, xFp32Reg1, maskReg);
                DataCopy(xFp32Addr2 + i * V_LENGTH, xFp32Reg2, maskReg);
            }
        }
    } else if constexpr (IsSameType<T, bfloat16_t>::value) {
        __VEC_SCOPE__
        {
            RegTensor<bfloat16_t> xBFp16Reg1, xBFp16Reg2;
            RegTensor<float> xFp32Reg1, vReg1, vRegTmp1, xFp32Reg2, vReg2, vRegTmp2;
            MaskReg maskReg;
            for (uint16_t i = 0; i < (uint16_t)repeatTimes; i++) {
                maskReg = UpdateMask<float>(sreg);
                DataCopy<bfloat16_t, LoadDist::DIST_UNPACK_B16>(xBFp16Reg1, xAddr1 + i * V_LENGTH);
                DataCopy<bfloat16_t, LoadDist::DIST_UNPACK_B16>(xBFp16Reg2, xAddr2 + i * V_LENGTH);
                Cast<float, bfloat16_t, castTraitB162B32>(xFp32Reg1, xBFp16Reg1, maskReg);
                Cast<float, bfloat16_t, castTraitB162B32>(xFp32Reg2, xBFp16Reg2, maskReg);
                Mul(vRegTmp1, xFp32Reg1, xFp32Reg1, maskReg);
                Mul(vRegTmp2, xFp32Reg2, xFp32Reg2, maskReg);
                Muls(vReg1, vRegTmp1, scalar, maskReg);
                Muls(vReg2, vRegTmp2, scalar, maskReg);
                DataCopy(dstAddr1 + i * V_LENGTH, vReg1, maskReg);
                DataCopy(dstAddr2 + i * V_LENGTH, vReg2, maskReg);
                DataCopy(xFp32Addr1 + i * V_LENGTH, xFp32Reg1, maskReg);
                DataCopy(xFp32Addr2 + i * V_LENGTH, xFp32Reg2, maskReg);
            }
        }
    } else {
        __VEC_SCOPE__
        {
            RegTensor<float> vReg1, vRegTmp1, vReg2, vRegTmp2;
            MaskReg maskReg;
            for (uint16_t i = 0; i < (uint16_t)repeatTimes; i++) {
                maskReg = UpdateMask<float>(sreg);
                DataCopy(vReg1, xAddr1 + i * V_LENGTH);
                DataCopy(vReg2, xAddr2 + i * V_LENGTH);
                Mul(vRegTmp1, vReg1, vReg1, maskReg);
                Mul(vRegTmp2, vReg2, vReg2, maskReg);
                Muls(vReg1, vRegTmp1, scalar, maskReg);
                Muls(vReg2, vRegTmp2, scalar, maskReg);
                DataCopy(dstAddr1 + i * V_LENGTH, vReg1, maskReg);
                DataCopy(dstAddr2 + i * V_LENGTH, vReg2, maskReg);
            }
        }
    }
}

/*!
 * dst = srcLocal * srcLocal + scalar
 *
 * @tparam T src dtype
 * @param dstLocal output Local Tensor
 * @param srcLocal input Local Tensor
 * @param scalar average num
 * @param count vector compute length
 * @return void
 */
template <typename T>
__aicore__ inline void ComputeInit(LocalTensor<float>& dstLocal, LocalTensor<T>& srcLocal, float scalar, uint32_t count)
{
    uint32_t calCount = count / 2;
    uint32_t sreg = (uint32_t)calCount;
    uint16_t repeatTimes = CeilDivision(calCount, V_LENGTH);

    __local_mem__ T* xAddr1 = (__ubuf__ T*)srcLocal.GetPhyAddr();
    __local_mem__ T* xAddr2 = (__ubuf__ T*)srcLocal.GetPhyAddr() + calCount;
    __local_mem__ float* dstAddr1 = (__ubuf__ float*)dstLocal.GetPhyAddr();
    __local_mem__ float* dstAddr2 = (__ubuf__ float*)dstLocal.GetPhyAddr() + calCount;

    if constexpr (IsSameType<T, half>::value) {
        __VEC_SCOPE__
        {
            RegTensor<half> xFp16Reg1, xFp16Reg2;
            RegTensor<float> xFp32Reg1, vReg1, vRegTmp1, xFp32Reg2, vReg2, vRegTmp2;
            MaskReg maskReg;
            for (uint16_t i = 0; i < (uint16_t)repeatTimes; i++) {
                maskReg = UpdateMask<float>(sreg);
                DataCopy<half, LoadDist::DIST_UNPACK_B16>(xFp16Reg1, xAddr1 + i * V_LENGTH);
                DataCopy<half, LoadDist::DIST_UNPACK_B16>(xFp16Reg2, xAddr2 + i * V_LENGTH);
                Cast<float, half, castTraitB162B32>(xFp32Reg1, xFp16Reg1, maskReg);
                Cast<float, half, castTraitB162B32>(xFp32Reg2, xFp16Reg2, maskReg);
                Mul(vRegTmp1, xFp32Reg1, xFp32Reg1, maskReg);
                Mul(vRegTmp2, xFp32Reg2, xFp32Reg2, maskReg);
                Muls(vReg1, vRegTmp1, scalar, maskReg);
                Muls(vReg2, vRegTmp2, scalar, maskReg);
                DataCopy(dstAddr1 + i * V_LENGTH, vReg1, maskReg);
                DataCopy(dstAddr2 + i * V_LENGTH, vReg2, maskReg);
            }
        }
    } else if constexpr (IsSameType<T, bfloat16_t>::value) {
        __VEC_SCOPE__
        {
            RegTensor<bfloat16_t> xBFp16Reg1, xBFp16Reg2;
            RegTensor<float> xFp32Reg1, vReg1, vRegTmp1, xFp32Reg2, vReg2, vRegTmp2;
            MaskReg maskReg;
            for (uint16_t i = 0; i < (uint16_t)repeatTimes; i++) {
                maskReg = UpdateMask<float>(sreg);
                DataCopy<bfloat16_t, LoadDist::DIST_UNPACK_B16>(xBFp16Reg1, xAddr1 + i * V_LENGTH);
                DataCopy<bfloat16_t, LoadDist::DIST_UNPACK_B16>(xBFp16Reg2, xAddr2 + i * V_LENGTH);
                Cast<float, bfloat16_t, castTraitB162B32>(xFp32Reg1, xBFp16Reg1, maskReg);
                Cast<float, bfloat16_t, castTraitB162B32>(xFp32Reg2, xBFp16Reg2, maskReg);
                Mul(vRegTmp1, xFp32Reg1, xFp32Reg1, maskReg);
                Mul(vRegTmp2, xFp32Reg2, xFp32Reg2, maskReg);
                Muls(vReg1, vRegTmp1, scalar, maskReg);
                Muls(vReg2, vRegTmp2, scalar, maskReg);
                DataCopy(dstAddr1 + i * V_LENGTH, vReg1, maskReg);
                DataCopy(dstAddr2 + i * V_LENGTH, vReg2, maskReg);
            }
        }
    } else {
        __VEC_SCOPE__
        {
            RegTensor<float> vReg1, vRegTmp1, vReg2, vRegTmp2;
            MaskReg maskReg;
            for (uint16_t i = 0; i < (uint16_t)repeatTimes; i++) {
                maskReg = UpdateMask<float>(sreg);
                DataCopy(vReg1, xAddr1 + i * V_LENGTH);
                DataCopy(vReg2, xAddr2 + i * V_LENGTH);
                Mul(vRegTmp1, vReg1, vReg1, maskReg);
                Mul(vRegTmp2, vReg2, vReg2, maskReg);
                Muls(vReg1, vRegTmp1, scalar, maskReg);
                Muls(vReg2, vRegTmp2, scalar, maskReg);
                DataCopy(dstAddr1 + i * V_LENGTH, vReg1, maskReg);
                DataCopy(dstAddr2 + i * V_LENGTH, vReg2, maskReg);
            }
        }
    }
}

/*!
 * rstd = 1 / sqrt(mean / n + epsilon)
 * @param rstdLocal store mean value Tensor
 * @param epsilon RmsNorm's attr
 * @param count The num of mean
 * @return void
 */
__aicore__ inline void ComputeRstd(LocalTensor<float>& rstdLocal, float epsilon, float avgFactor, uint32_t count)
{
    __local_mem__ float* rstdLocalAddr = (__ubuf__ float*)rstdLocal.GetPhyAddr();

    uint32_t calCount = count;
    uint32_t sreg = (uint32_t)calCount;
    uint16_t repeatTimes = CeilDivision(calCount, V_LENGTH);
    __VEC_SCOPE__
    {
        RegTensor<float> vReg, srcReg, dstReg;
        MaskReg maskReg;
        for (uint16_t i = 0; i < (uint16_t)repeatTimes; i++) {
            maskReg = UpdateMask<float>(sreg);
            DataCopy(srcReg, rstdLocalAddr + i * V_LENGTH);
            Muls(srcReg, srcReg, avgFactor, maskReg);
            Adds(dstReg, srcReg, epsilon, maskReg);
            Sqrt(vReg, dstReg, maskReg);
            Duplicate(srcReg, float(1.0), maskReg);
            Div(dstReg, srcReg, vReg, maskReg);
            DataCopy(rstdLocalAddr + i * V_LENGTH, dstReg, maskReg);
        }
    }
}

/*!
 * compute yLocal = xLocal * rstd * gammaLocal
 *
 * @param xLocal input xLocal
 * @param gammaLocal input gammaLocal
 * @param yLocal output yLocal
 * @param rstd input rstd
 * @param count vector commpute length
 * @return void
 */
template <typename DX, typename DG>
__aicore__ inline void ComputeY(
    LocalTensor<float>& xLocal, LocalTensor<DG>& gammaLocal, LocalTensor<DX>& yLocal, LocalTensor<float>& rstdLocal,
    uint32_t offset, uint32_t count)
{
    uint32_t calCount = count / 2;
    uint32_t sreg = (uint32_t)calCount;
    uint16_t repeatTimes = CeilDivision(calCount, V_LENGTH);

    __local_mem__ float* xAddr1 = (__ubuf__ float*)xLocal.GetPhyAddr();
    __local_mem__ float* xAddr2 = (__ubuf__ float*)xLocal.GetPhyAddr() + calCount;
    __local_mem__ DG* gammaAddr1 = (__ubuf__ DG*)gammaLocal.GetPhyAddr();
    __local_mem__ DG* gammaAddr2 = (__ubuf__ DG*)gammaLocal.GetPhyAddr() + calCount;
    __local_mem__ float* rstdAddr = (__ubuf__ float*)rstdLocal.GetPhyAddr();
    __local_mem__ DX* yAddr1 = (__ubuf__ DX*)yLocal.GetPhyAddr();
    __local_mem__ DX* yAddr2 = (__ubuf__ DX*)yLocal.GetPhyAddr() + calCount;

    if constexpr (!IsSameType<DX, float>::value && !IsSameType<DG, float>::value) {
        __VEC_SCOPE__
        {
            RegTensor<DX> yB16Reg1, yB16Reg2;
            RegTensor<DG> gammaReg1, gammaReg2;
            RegTensor<float> rstdReg;
            RegTensor<float> xReg1, dst1Reg, gammaFp32Reg1, yReg1;
            RegTensor<float> xReg2, dst2Reg, gammaFp32Reg2, yReg2;
            MaskReg maskReg;
            DataCopy<float, LoadDist::DIST_BRC_B32>(rstdReg, rstdAddr + offset);
            for (uint16_t i = 0; i < (uint16_t)repeatTimes; i++) {
                maskReg = UpdateMask<float>(sreg);
                DataCopy(xReg1, xAddr1 + i * V_LENGTH);
                DataCopy(xReg2, xAddr2 + i * V_LENGTH);
                DataCopy<DG, LoadDist::DIST_UNPACK_B16>(gammaReg1, gammaAddr1 + i * V_LENGTH);
                DataCopy<DG, LoadDist::DIST_UNPACK_B16>(gammaReg2, gammaAddr2 + i * V_LENGTH);
                Cast<float, DG, castTraitB162B32>(gammaFp32Reg1, gammaReg1, maskReg);
                Cast<float, DG, castTraitB162B32>(gammaFp32Reg2, gammaReg2, maskReg);
                Mul(dst1Reg, xReg1, rstdReg, maskReg);
                Mul(dst2Reg, xReg2, rstdReg, maskReg);
                Mul(yReg1, dst1Reg, gammaFp32Reg1, maskReg);
                Mul(yReg2, dst2Reg, gammaFp32Reg2, maskReg);
                Cast<DX, float, castTraitB322B16>(yB16Reg1, yReg1, maskReg);
                Cast<DX, float, castTraitB322B16>(yB16Reg2, yReg2, maskReg);
                DataCopy<DX, StoreDist::DIST_PACK_B32>(yAddr1 + i * V_LENGTH, yB16Reg1, maskReg);
                DataCopy<DX, StoreDist::DIST_PACK_B32>(yAddr2 + i * V_LENGTH, yB16Reg2, maskReg);
            }
        }
    } else if constexpr (!IsSameType<DX, float>::value and IsSameType<DG, float>::value) {
        __VEC_SCOPE__
        {
            RegTensor<DX> yB16Reg1, yB16Reg2;
            RegTensor<DG> gammaReg1, gammaReg2;
            RegTensor<float> rstdReg;
            RegTensor<float> xReg1, dst1Reg, yReg1;
            RegTensor<float> xReg2, dst2Reg, yReg2;
            MaskReg maskReg;
            DataCopy<float, LoadDist::DIST_BRC_B32>(rstdReg, rstdAddr + offset);
            for (uint16_t i = 0; i < (uint16_t)repeatTimes; i++) {
                maskReg = UpdateMask<float>(sreg);
                DataCopy(xReg1, xAddr1 + i * V_LENGTH);
                DataCopy(xReg2, xAddr2 + i * V_LENGTH);
                DataCopy(gammaReg1, gammaAddr1 + i * V_LENGTH);
                DataCopy(gammaReg2, gammaAddr2 + i * V_LENGTH);
                Mul(dst1Reg, xReg1, rstdReg, maskReg);
                Mul(dst2Reg, xReg2, rstdReg, maskReg);
                Mul(yReg1, dst1Reg, gammaReg1, maskReg);
                Mul(yReg2, dst2Reg, gammaReg2, maskReg);
                Cast<DX, float, castTraitB322B16>(yB16Reg1, yReg1, maskReg);
                Cast<DX, float, castTraitB322B16>(yB16Reg2, yReg2, maskReg);
                DataCopy<DX, StoreDist::DIST_PACK_B32>(yAddr1 + i * V_LENGTH, yB16Reg1, maskReg);
                DataCopy<DX, StoreDist::DIST_PACK_B32>(yAddr2 + i * V_LENGTH, yB16Reg2, maskReg);
            }
        }
    } else {
        __VEC_SCOPE__
        {
            RegTensor<float> rstdReg;
            RegTensor<float> xReg1, gammaReg1, yReg1, vRegTmp1;
            RegTensor<float> xReg2, gammaReg2, yReg2, vRegTmp2;
            MaskReg maskReg;
            DataCopy<float, LoadDist::DIST_BRC_B32>(rstdReg, rstdAddr + offset);
            for (uint16_t i = 0; i < (uint16_t)repeatTimes; i++) {
                maskReg = UpdateMask<float>(sreg);
                DataCopy(xReg1, xAddr1 + i * V_LENGTH);
                DataCopy(xReg2, xAddr2 + i * V_LENGTH);
                DataCopy(gammaReg1, gammaAddr1 + i * V_LENGTH);
                DataCopy(gammaReg2, gammaAddr2 + i * V_LENGTH);
                Mul(vRegTmp1, xReg1, rstdReg, maskReg);
                Mul(vRegTmp2, xReg2, rstdReg, maskReg);
                Mul(yReg1, vRegTmp1, gammaReg1, maskReg);
                Mul(yReg2, vRegTmp2, gammaReg2, maskReg);
                DataCopy(yAddr1 + i * V_LENGTH, yReg1, maskReg);
                DataCopy(yAddr2 + i * V_LENGTH, yReg2, maskReg);
            }
        }
    }
}
/*!
 * compute multi N yLocal = xLocal * rstd * gammaLocal
 *
 * @param xLocal input xLocal
 * @param gammaLocal input gammaLocal
 * @param yLocal output yLocal
 * @param rstd input rstd
 * @param count vector commpute length
 * @return void
 */
template <typename DX, typename DG>
__aicore__ inline void ComputeYMultiN(
    LocalTensor<float>& xLocal, LocalTensor<DG>& gammaLocal, LocalTensor<DX>& yLocal, LocalTensor<float>& rstdLocal,
    uint32_t offset, uint32_t count, uint32_t curRows)
{
    uint32_t calCount = count / 2;
    uint16_t repeatTimes = CeilDivision(calCount, V_LENGTH);

    __local_mem__ float* xAddr1 = (__ubuf__ float*)xLocal.GetPhyAddr();
    __local_mem__ float* xAddr2 = (__ubuf__ float*)xLocal.GetPhyAddr() + calCount;
    __local_mem__ DG* gammaAddr1 = (__ubuf__ DG*)gammaLocal.GetPhyAddr();
    __local_mem__ DG* gammaAddr2 = (__ubuf__ DG*)gammaLocal.GetPhyAddr() + calCount;
    __local_mem__ float* rstdAddr = (__ubuf__ float*)rstdLocal.GetPhyAddr();
    __local_mem__ DX* yAddr1 = (__ubuf__ DX*)yLocal.GetPhyAddr();
    __local_mem__ DX* yAddr2 = (__ubuf__ DX*)yLocal.GetPhyAddr() + calCount;

    if constexpr (!IsSameType<DX, float>::value && !IsSameType<DG, float>::value) {
        __VEC_SCOPE__
        {
            for (uint16_t row = 0; row < static_cast<uint16_t>(curRows); row++) {
                uint32_t sreg = (uint32_t)calCount;
                RegTensor<DX> yB16Reg1, yB16Reg2;
                RegTensor<DG> gammaReg1, gammaReg2;
                RegTensor<float> rstdReg;
                RegTensor<float> xReg1, dst1Reg, gammaFp32Reg1, yReg1;
                RegTensor<float> xReg2, dst2Reg, gammaFp32Reg2, yReg2;
                MaskReg maskReg;
                DataCopy<float, LoadDist::DIST_BRC_B32>(rstdReg, rstdAddr + offset);
                for (uint16_t i = 0; i < (uint16_t)repeatTimes; i++) {
                    maskReg = UpdateMask<float>(sreg);
                    DataCopy(xReg1, xAddr1 + i * V_LENGTH);
                    DataCopy(xReg2, xAddr2 + i * V_LENGTH);
                    DataCopy<DG, LoadDist::DIST_UNPACK_B16>(gammaReg1, gammaAddr1 + i * V_LENGTH);
                    DataCopy<DG, LoadDist::DIST_UNPACK_B16>(gammaReg2, gammaAddr2 + i * V_LENGTH);
                    Cast<float, DG, castTraitB162B32>(gammaFp32Reg1, gammaReg1, maskReg);
                    Cast<float, DG, castTraitB162B32>(gammaFp32Reg2, gammaReg2, maskReg);
                    Mul(dst1Reg, xReg1, rstdReg, maskReg);
                    Mul(dst2Reg, xReg2, rstdReg, maskReg);
                    Mul(yReg1, dst1Reg, gammaFp32Reg1, maskReg);
                    Mul(yReg2, dst2Reg, gammaFp32Reg2, maskReg);
                    Cast<DX, float, castTraitB322B16>(yB16Reg1, yReg1, maskReg);
                    Cast<DX, float, castTraitB322B16>(yB16Reg2, yReg2, maskReg);
                    DataCopy<DX, StoreDist::DIST_PACK_B32>(yAddr1 + i * V_LENGTH, yB16Reg1, maskReg);
                    DataCopy<DX, StoreDist::DIST_PACK_B32>(yAddr2 + i * V_LENGTH, yB16Reg2, maskReg);
                }
                offset++;
                xAddr1 += count;
                xAddr2 += count;
                yAddr1 += count;
                yAddr2 += count;
            }
        }
    } else if constexpr (!IsSameType<DX, float>::value and IsSameType<DG, float>::value) {
        __VEC_SCOPE__
        {
            for (uint16_t row = 0; row < static_cast<uint16_t>(curRows); row++) {
                uint32_t sreg = (uint32_t)calCount;
                RegTensor<DX> yB16Reg1, yB16Reg2;
                RegTensor<DG> gammaReg1, gammaReg2;
                RegTensor<float> rstdReg;
                RegTensor<float> xReg1, dst1Reg, yReg1;
                RegTensor<float> xReg2, dst2Reg, yReg2;
                MaskReg maskReg;
                DataCopy<float, LoadDist::DIST_BRC_B32>(rstdReg, rstdAddr + offset);
                for (uint16_t i = 0; i < (uint16_t)repeatTimes; i++) {
                    maskReg = UpdateMask<float>(sreg);
                    DataCopy(xReg1, xAddr1 + i * V_LENGTH);
                    DataCopy(xReg2, xAddr2 + i * V_LENGTH);
                    DataCopy(gammaReg1, gammaAddr1 + i * V_LENGTH);
                    DataCopy(gammaReg2, gammaAddr2 + i * V_LENGTH);
                    Mul(dst1Reg, xReg1, rstdReg, maskReg);
                    Mul(dst2Reg, xReg2, rstdReg, maskReg);
                    Mul(yReg1, dst1Reg, gammaReg1, maskReg);
                    Mul(yReg2, dst2Reg, gammaReg2, maskReg);
                    Cast<DX, float, castTraitB322B16>(yB16Reg1, yReg1, maskReg);
                    Cast<DX, float, castTraitB322B16>(yB16Reg2, yReg2, maskReg);
                    DataCopy<DX, StoreDist::DIST_PACK_B32>(yAddr1 + i * V_LENGTH, yB16Reg1, maskReg);
                    DataCopy<DX, StoreDist::DIST_PACK_B32>(yAddr2 + i * V_LENGTH, yB16Reg2, maskReg);
                }
                offset++;
                xAddr1 += count;
                xAddr2 += count;
                yAddr1 += count;
                yAddr2 += count;
            }
        }
    } else {
        __VEC_SCOPE__
        {
            for (uint16_t row = 0; row < static_cast<uint16_t>(curRows); row++) {
                uint32_t sreg = (uint32_t)calCount;
                RegTensor<float> rstdReg;
                RegTensor<float> xReg1, gammaReg1, yReg1, vRegTmp1;
                RegTensor<float> xReg2, gammaReg2, yReg2, vRegTmp2;
                MaskReg maskReg;
                DataCopy<float, LoadDist::DIST_BRC_B32>(rstdReg, rstdAddr + offset);
                for (uint16_t i = 0; i < (uint16_t)repeatTimes; i++) {
                    maskReg = UpdateMask<float>(sreg);
                    DataCopy(xReg1, xAddr1 + i * V_LENGTH);
                    DataCopy(xReg2, xAddr2 + i * V_LENGTH);
                    DataCopy(gammaReg1, gammaAddr1 + i * V_LENGTH);
                    DataCopy(gammaReg2, gammaAddr2 + i * V_LENGTH);
                    Mul(vRegTmp1, xReg1, rstdReg, maskReg);
                    Mul(vRegTmp2, xReg2, rstdReg, maskReg);
                    Mul(yReg1, vRegTmp1, gammaReg1, maskReg);
                    Mul(yReg2, vRegTmp2, gammaReg2, maskReg);
                    DataCopy(yAddr1 + i * V_LENGTH, yReg1, maskReg);
                    DataCopy(yAddr2 + i * V_LENGTH, yReg2, maskReg);
                }
                offset++;
                xAddr1 += count;
                xAddr2 += count;
                yAddr1 += count;
                yAddr2 += count;
            }
        }
    }
}

/*!
 * compute yLocal = xLocal * rstd * gammaLocal
 *
 * @param xLocal input xLocal
 * @param gammaLocal input gammaLocal
 * @param yLocal output yLocal
 * @param rstd input rstd
 * @param count vector commpute length
 * @return void
 */
template <typename DX, typename DG>
__aicore__ inline void ComputeLatterY(
    LocalTensor<DX>& xLocal, LocalTensor<DG>& gammaLocal, LocalTensor<DX>& yLocal, LocalTensor<float>& rstdLocal,
    uint32_t offset, uint32_t count)
{
    uint32_t calCount = count / 2;
    uint32_t sreg = (uint32_t)calCount;
    uint16_t repeatTimes = CeilDivision(calCount, V_LENGTH);

    __local_mem__ DX* xAddr1 = (__ubuf__ DX*)xLocal.GetPhyAddr();
    __local_mem__ DX* xAddr2 = (__ubuf__ DX*)xLocal.GetPhyAddr() + calCount;
    __local_mem__ DG* gammaAddr1 = (__ubuf__ DG*)gammaLocal.GetPhyAddr();
    __local_mem__ DG* gammaAddr2 = (__ubuf__ DG*)gammaLocal.GetPhyAddr() + calCount;
    __local_mem__ float* srcAddr2 = (__ubuf__ float*)rstdLocal.GetPhyAddr();
    __local_mem__ DX* yAddr1 = (__ubuf__ DX*)yLocal.GetPhyAddr();
    __local_mem__ DX* yAddr2 = (__ubuf__ DX*)yLocal.GetPhyAddr() + calCount;

    if constexpr (!IsSameType<DX, float>::value and !IsSameType<DG, float>::value) {
        __VEC_SCOPE__
        {
            RegTensor<DX> xB16Reg1, yB16Reg1, xB16Reg2, yB16Reg2;
            RegTensor<DG> gammaReg1, gammaReg2;
            RegTensor<float> rstdReg;
            RegTensor<float> xReg1, dst1Reg, gammaFp32Reg1, yReg1;
            RegTensor<float> xReg2, dst2Reg, gammaFp32Reg2, yReg2;
            MaskReg maskReg;
            DataCopy<float, LoadDist::DIST_BRC_B32>(rstdReg, srcAddr2 + offset);
            for (uint16_t i = 0; i < (uint16_t)repeatTimes; i++) {
                maskReg = UpdateMask<float>(sreg);
                DataCopy<DX, LoadDist::DIST_UNPACK_B16>(xB16Reg1, xAddr1 + i * V_LENGTH);
                DataCopy<DX, LoadDist::DIST_UNPACK_B16>(xB16Reg2, xAddr2 + i * V_LENGTH);
                DataCopy<DG, LoadDist::DIST_UNPACK_B16>(gammaReg1, gammaAddr1 + i * V_LENGTH);
                DataCopy<DG, LoadDist::DIST_UNPACK_B16>(gammaReg2, gammaAddr2 + i * V_LENGTH);
                Cast<float, DG, castTraitB162B32>(gammaFp32Reg1, gammaReg1, maskReg);
                Cast<float, DG, castTraitB162B32>(gammaFp32Reg2, gammaReg2, maskReg);
                Cast<float, DX, castTraitB162B32>(xReg1, xB16Reg1, maskReg);
                Cast<float, DX, castTraitB162B32>(xReg2, xB16Reg2, maskReg);
                Mul(dst1Reg, xReg1, rstdReg, maskReg);
                Mul(dst2Reg, xReg2, rstdReg, maskReg);
                Mul(yReg1, dst1Reg, gammaFp32Reg1, maskReg);
                Mul(yReg2, dst2Reg, gammaFp32Reg2, maskReg);
                Cast<DX, float, castTraitB322B16>(yB16Reg1, yReg1, maskReg);
                Cast<DX, float, castTraitB322B16>(yB16Reg2, yReg2, maskReg);
                DataCopy<DX, StoreDist::DIST_PACK_B32>(yAddr1 + i * V_LENGTH, yB16Reg1, maskReg);
                DataCopy<DX, StoreDist::DIST_PACK_B32>(yAddr2 + i * V_LENGTH, yB16Reg2, maskReg);
            }
        }
    } else if constexpr (!IsSameType<DX, float>::value and IsSameType<DG, float>::value) {
        __VEC_SCOPE__
        {
            RegTensor<DX> xB16Reg1, yB16Reg1, xB16Reg2, yB16Reg2;
            RegTensor<float> rstdReg;
            RegTensor<float> xReg1, dst1Reg, gammaFp32Reg1, yReg1;
            RegTensor<float> xReg2, dst2Reg, gammaFp32Reg2, yReg2;
            MaskReg maskReg;
            DataCopy<float, LoadDist::DIST_BRC_B32>(rstdReg, srcAddr2 + offset);
            for (uint16_t i = 0; i < (uint16_t)repeatTimes; i++) {
                maskReg = UpdateMask<float>(sreg);
                DataCopy<DX, LoadDist::DIST_UNPACK_B16>(xB16Reg1, xAddr1 + i * V_LENGTH);
                DataCopy<DX, LoadDist::DIST_UNPACK_B16>(xB16Reg2, xAddr2 + i * V_LENGTH);
                DataCopy(gammaFp32Reg1, gammaAddr1 + i * V_LENGTH);
                DataCopy(gammaFp32Reg2, gammaAddr2 + i * V_LENGTH);
                Cast<float, DX, castTraitB162B32>(xReg1, xB16Reg1, maskReg);
                Cast<float, DX, castTraitB162B32>(xReg2, xB16Reg2, maskReg);
                Mul(dst1Reg, xReg1, rstdReg, maskReg);
                Mul(dst2Reg, xReg2, rstdReg, maskReg);
                Mul(yReg1, dst1Reg, gammaFp32Reg1, maskReg);
                Mul(yReg2, dst2Reg, gammaFp32Reg2, maskReg);
                Cast<DX, float, castTraitB322B16>(yB16Reg1, yReg1, maskReg);
                Cast<DX, float, castTraitB322B16>(yB16Reg2, yReg2, maskReg);
                DataCopy<DX, StoreDist::DIST_PACK_B32>(yAddr1 + i * V_LENGTH, yB16Reg1, maskReg);
                DataCopy<DX, StoreDist::DIST_PACK_B32>(yAddr2 + i * V_LENGTH, yB16Reg2, maskReg);
            }
        }
    } else {
        __VEC_SCOPE__
        {
            RegTensor<float> rstdReg;
            RegTensor<float> xReg1, gammaReg1, yReg1, vRegTmp1;
            RegTensor<float> xReg2, gammaReg2, yReg2, vRegTmp2;
            MaskReg maskReg;
            DataCopy<float, LoadDist::DIST_BRC_B32>(rstdReg, srcAddr2 + offset);
            for (uint16_t i = 0; i < (uint16_t)repeatTimes; i++) {
                maskReg = UpdateMask<float>(sreg);
                DataCopy(xReg1, xAddr1 + i * V_LENGTH);
                DataCopy(xReg2, xAddr2 + i * V_LENGTH);
                DataCopy(gammaReg1, gammaAddr1 + i * V_LENGTH);
                DataCopy(gammaReg2, gammaAddr2 + i * V_LENGTH);
                Mul(vRegTmp1, xReg1, rstdReg, maskReg);
                Mul(vRegTmp2, xReg2, rstdReg, maskReg);
                Mul(yReg1, vRegTmp1, gammaReg1, maskReg);
                Mul(yReg2, vRegTmp2, gammaReg2, maskReg);
                DataCopy(yAddr1 + i * V_LENGTH, yReg1, maskReg);
                DataCopy(yAddr2 + i * V_LENGTH, yReg2, maskReg);
            }
        }
    }
}

/*!
 * The num of each level elements is 256, ReduceSum these elements and store to the next level.
 * @param level1Local level1Tensor
 * @param level2Local level2Tensor
 * @param level3Local level3Tensor
 * @param level1 level1 elements
 * @param level2 level2 elements
 * @param level3 level3 elements
 * @return void
 */
__aicore__ inline void ComputeMultiLevelReduce(
    LocalTensor<float>& level1Local, LocalTensor<float>& level2Local, LocalTensor<float>& level3Local, uint32_t& level1,
    uint32_t& level2, uint32_t& level3)
{
    if (level1 == NormCommon::ONCE_VECTOR_SIZE) {
        LevelMergeRstd<false>(level2Local, level1Local, level2, NormCommon::ONCE_VECTOR_SIZE);
        level1 = 0;
        level2 += 1;
    }
    if (level2 == NormCommon::ONCE_VECTOR_SIZE) {
        LevelMergeRstd<false>(level3Local, level2Local, level3, NormCommon::ONCE_VECTOR_SIZE);
        level2 = 0;
        level3 += 1;
    }
}

__aicore__ inline void ComputeSum(
    LocalTensor<float>& dstLocal, LocalTensor<float>& srcLocal, uint32_t offset, uint32_t count)
{
    uint32_t meanTile = count;
    uint32_t meanSreg = meanTile;

    __local_mem__ float* srcAddr = (__ubuf__ float*)srcLocal.GetPhyAddr();
    __local_mem__ float* dstAddr = (__ubuf__ float*)dstLocal.GetPhyAddr();

    __VEC_SCOPE__
    {
        RegTensor<float> vReg, vMean;
        MaskReg pregLoop;
        MaskReg pregMerge = CreateMask<float, MaskPattern::VL1>();
        {
            pregLoop = UpdateMask<float>(meanSreg);
            DataCopy(vReg, srcAddr + 0);
            ReduceSum(vMean, vReg, pregLoop);
            DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(dstAddr + offset, vMean, pregMerge);
        }
    }
}

/*!
 * ReduceSum impl by half add.
 * @param dstLocal dst Tensor
 * @param srcLocal src Tensor
 * @param workLocal  temp Tensor
 * @param offset dst offset
 * @param count count aligned compute elements.
 * @param powerSplit 2 ** k = powerSplit
 * @return void
 */
__aicore__ inline void ReduceSumImpl(
    LocalTensor<float>& dstLocal, LocalTensor<float>& srcLocal, LocalTensor<float>& workLocal, uint32_t offset,
    uint32_t count, uint32_t powerSplit)
{
    uint32_t remainTile = count - powerSplit;
    uint32_t remainSreg = remainTile;
    uint32_t remainRepeats = remainTile / (2 * V_LENGTH);

    uint32_t masterTile = powerSplit - remainTile;
    uint32_t masterSreg = masterTile;
    uint16_t masterRepeats = masterTile / (2 * V_LENGTH);

    uint32_t mergeTile = powerSplit / (2 * V_LENGTH);
    uint32_t mergeSreg = mergeTile;
    uint32_t mergeRepeats = mergeTile / (2 * V_LENGTH);

    uint32_t meanTile = mergeRepeats == 0 ? mergeTile : mergeRepeats;
    uint32_t meanSreg = meanTile;

    __local_mem__ float* mainAddr = (__ubuf__ float*)srcLocal.GetPhyAddr();
    __local_mem__ float* tailAddr = (__ubuf__ float*)srcLocal.GetPhyAddr() + int64_t(powerSplit);
    __local_mem__ float* masterAddr = (__ubuf__ float*)srcLocal.GetPhyAddr() + int64_t(remainTile);
    __local_mem__ float* workAddr = (__ubuf__ float*)workLocal.GetPhyAddr();
    __local_mem__ float* dstAddr = (__ubuf__ float*)dstLocal.GetPhyAddr();

    __VEC_SCOPE__
    {
        RegTensor<float> mainA, mainB, tailA, tailB, vMean;
        MaskReg pregMain = CreateMask<float, MaskPattern::ALL>();
        MaskReg pregMerge = CreateMask<float, MaskPattern::VL1>();
        MaskReg pregLoop;

        for (uint16_t i = 0; i < (uint16_t)remainRepeats; ++i) {
            pregLoop = UpdateMask<float>(remainSreg);
            DataCopy(mainA, mainAddr + (i * 2 + 0) * V_LENGTH);
            DataCopy(mainB, mainAddr + (i * 2 + 1) * V_LENGTH);
            DataCopy(tailA, tailAddr + (i * 2 + 0) * V_LENGTH);
            DataCopy(tailB, tailAddr + (i * 2 + 1) * V_LENGTH);

            Add(mainA, mainA, tailA, pregLoop);
            Add(mainB, mainB, tailB, pregLoop);
            Add(mainA, mainA, mainB, pregLoop);
            ReduceSum(vMean, mainA, pregLoop);
            DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(workAddr + i, vMean, pregMerge);
        }
        for (uint16_t i = 0; i < (uint16_t)masterRepeats; ++i) {
            pregLoop = UpdateMask<float>(masterSreg);
            DataCopy(mainA, masterAddr + (i * 2 + 0) * V_LENGTH);
            DataCopy(mainB, masterAddr + (i * 2 + 1) * V_LENGTH);
            Add(mainA, mainA, mainB, pregLoop);
            ReduceSum(vMean, mainA, pregLoop);
            DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(workAddr + remainRepeats + i, vMean, pregMerge);
        }
        LocalMemBar<MemType::VEC_STORE, MemType::VEC_LOAD>();
        for (uint16_t i = 0; i < (uint16_t)mergeRepeats; ++i) {
            pregLoop = UpdateMask<float>(mergeSreg);
            DataCopy(mainA, workAddr + (i * 2 + 0) * V_LENGTH);
            DataCopy(mainB, workAddr + (i * 2 + 1) * V_LENGTH);
            Add(mainA, mainA, mainB, pregLoop);
            ReduceSum(vMean, mainA, pregLoop);
            DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(workAddr + i, vMean, pregMerge);
        }
        LocalMemBar<MemType::VEC_STORE, MemType::VEC_LOAD>();
        {
            pregLoop = UpdateMask<float>(meanSreg);
            DataCopy(mainA, workAddr + 0);
            ReduceSum(vMean, mainA, pregLoop);
            DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(dstAddr + offset, vMean, pregMerge);
        }
    }
}

template <typename T>
__aicore__ inline void LoadForHandleRemainV1(
    __local_mem__ T* mainAddr, __local_mem__ T* tailAddr, uint16_t offset1, uint16_t offset2, RegTensor<float>& mainA,
    RegTensor<float>& mainB, RegTensor<float>& tailA, RegTensor<float>& tailB, MaskReg& pregLoop,
    __local_mem__ float* xFp32MainAddr, __local_mem__ float* xFp32TailAddr)
{
    if constexpr (IsSameType<T, half>::value) {
        RegTensor<half> xFp16MainA, xFp16MainB, xFp16TailA, xFp16TailB;
        DataCopy<half, LoadDist::DIST_UNPACK_B16>(xFp16MainA, mainAddr + offset1);
        DataCopy<half, LoadDist::DIST_UNPACK_B16>(xFp16MainB, mainAddr + offset2);
        DataCopy<half, LoadDist::DIST_UNPACK_B16>(xFp16TailA, tailAddr + offset1);
        DataCopy<half, LoadDist::DIST_UNPACK_B16>(xFp16TailB, tailAddr + offset2);
        Cast<float, half, castTraitB162B32>(mainA, xFp16MainA, pregLoop);
        Cast<float, half, castTraitB162B32>(mainB, xFp16MainB, pregLoop);
        Cast<float, half, castTraitB162B32>(tailA, xFp16TailA, pregLoop);
        Cast<float, half, castTraitB162B32>(tailB, xFp16TailB, pregLoop);
        DataCopy(xFp32MainAddr + offset1, mainA, pregLoop);
        DataCopy(xFp32MainAddr + offset2, mainB, pregLoop);
        DataCopy(xFp32TailAddr + offset1, tailA, pregLoop);
        DataCopy(xFp32TailAddr + offset2, tailB, pregLoop);
        Mul(mainA, mainA, mainA, pregLoop);
        Mul(mainB, mainB, mainB, pregLoop);
        Mul(tailA, tailA, tailA, pregLoop);
        Mul(tailB, tailB, tailB, pregLoop);
    } else if constexpr (IsSameType<T, bfloat16_t>::value) {
        RegTensor<bfloat16_t> xBFp16MainA, xBFp16MainB, xBFp16TailA, xBFp16TailB;
        DataCopy<bfloat16_t, LoadDist::DIST_UNPACK_B16>(xBFp16MainA, mainAddr + offset1);
        DataCopy<bfloat16_t, LoadDist::DIST_UNPACK_B16>(xBFp16MainB, mainAddr + offset2);
        DataCopy<bfloat16_t, LoadDist::DIST_UNPACK_B16>(xBFp16TailA, tailAddr + offset1);
        DataCopy<bfloat16_t, LoadDist::DIST_UNPACK_B16>(xBFp16TailB, tailAddr + offset2);
        Cast<float, bfloat16_t, castTraitB162B32>(mainA, xBFp16MainA, pregLoop);
        Cast<float, bfloat16_t, castTraitB162B32>(mainB, xBFp16MainB, pregLoop);
        Cast<float, bfloat16_t, castTraitB162B32>(tailA, xBFp16TailA, pregLoop);
        Cast<float, bfloat16_t, castTraitB162B32>(tailB, xBFp16TailB, pregLoop);
        DataCopy(xFp32MainAddr + offset1, mainA, pregLoop);
        DataCopy(xFp32MainAddr + offset2, mainB, pregLoop);
        DataCopy(xFp32TailAddr + offset1, tailA, pregLoop);
        DataCopy(xFp32TailAddr + offset2, tailB, pregLoop);
        Mul(mainA, mainA, mainA, pregLoop);
        Mul(mainB, mainB, mainB, pregLoop);
        Mul(tailA, tailA, tailA, pregLoop);
        Mul(tailB, tailB, tailB, pregLoop);
    } else {
        DataCopy(mainA, mainAddr + offset1);
        DataCopy(mainB, mainAddr + offset2);
        DataCopy(tailA, tailAddr + offset1);
        DataCopy(tailB, tailAddr + offset2);
        Mul(mainA, mainA, mainA, pregLoop);
        Mul(mainB, mainB, mainB, pregLoop);
        Mul(tailA, tailA, tailA, pregLoop);
        Mul(tailB, tailB, tailB, pregLoop);
    }
}

template <typename T>
__aicore__ inline void LoadForHandleMasterV1(
    __local_mem__ T* masterAddr, uint16_t offset1, uint16_t offset2, RegTensor<float>& mainA, RegTensor<float>& mainB,
    MaskReg& pregLoop, __local_mem__ float* xFp32MasterAddr)
{
    if constexpr (IsSameType<T, half>::value) {
        RegTensor<half> xFp16MainA, xFp16MainB;
        DataCopy<half, LoadDist::DIST_UNPACK_B16>(xFp16MainA, masterAddr + offset1);
        DataCopy<half, LoadDist::DIST_UNPACK_B16>(xFp16MainB, masterAddr + offset2);
        Cast<float, half, castTraitB162B32>(mainA, xFp16MainA, pregLoop);
        Cast<float, half, castTraitB162B32>(mainB, xFp16MainB, pregLoop);
        DataCopy(xFp32MasterAddr + offset1, mainA, pregLoop);
        DataCopy(xFp32MasterAddr + offset2, mainB, pregLoop);
        Mul(mainA, mainA, mainA, pregLoop);
        Mul(mainB, mainB, mainB, pregLoop);
    } else if constexpr (IsSameType<T, bfloat16_t>::value) {
        RegTensor<bfloat16_t> xBFp16MainA, xBFp16MainB;
        DataCopy<bfloat16_t, LoadDist::DIST_UNPACK_B16>(xBFp16MainA, masterAddr + offset1);
        DataCopy<bfloat16_t, LoadDist::DIST_UNPACK_B16>(xBFp16MainB, masterAddr + offset2);
        Cast<float, bfloat16_t, castTraitB162B32>(mainA, xBFp16MainA, pregLoop);
        Cast<float, bfloat16_t, castTraitB162B32>(mainB, xBFp16MainB, pregLoop);
        DataCopy(xFp32MasterAddr + offset1, mainA, pregLoop);
        DataCopy(xFp32MasterAddr + offset2, mainB, pregLoop);
        Mul(mainA, mainA, mainA, pregLoop);
        Mul(mainB, mainB, mainB, pregLoop);
    } else {
        DataCopy(mainA, masterAddr + offset1);
        DataCopy(mainB, masterAddr + offset2);
        Mul(mainA, mainA, mainA, pregLoop);
        Mul(mainB, mainB, mainB, pregLoop);
    }
}

template <typename T>
__aicore__ inline void ComputeFormerImplV1(
    LocalTensor<T>& xLocal, LocalTensor<float>& xFp32, LocalTensor<float>& workLocal, LocalTensor<float>& rstdLocal,
    float avgFactor, float epsilon, uint32_t offset, uint32_t count, uint32_t powerSplit)
{
    uint32_t remainTile = count - powerSplit;
    uint32_t remainSreg = remainTile;
    uint16_t remainRepeats = remainTile / (2 * V_LENGTH);

    uint32_t masterTile = powerSplit - remainTile;
    uint32_t masterSreg = masterTile;
    uint16_t masterRepeats = masterTile / (2 * V_LENGTH);

    uint32_t mergeTile = powerSplit / (2 * V_LENGTH);
    uint32_t mergeSreg = mergeTile;
    uint16_t mergeRepeats = mergeTile / (2 * V_LENGTH);

    uint32_t meanTile = mergeRepeats == 0 ? mergeTile : mergeRepeats;
    uint32_t meanSreg = meanTile;

    __local_mem__ T* mainAddr = (__ubuf__ T*)xLocal.GetPhyAddr();
    __local_mem__ T* tailAddr = (__ubuf__ T*)xLocal.GetPhyAddr() + int64_t(powerSplit);
    __local_mem__ T* masterAddr = (__ubuf__ T*)xLocal.GetPhyAddr() + int64_t(remainTile);
    __local_mem__ float *xFp32MainAddr, *xFp32TailAddr, *xFp32MasterAddr;
    if constexpr (is_same<T, half>::value || is_same<T, bfloat16_t>::value) {
        xFp32MainAddr = (__ubuf__ float*)xFp32.GetPhyAddr();
        xFp32TailAddr = (__ubuf__ float*)xFp32.GetPhyAddr() + int64_t(powerSplit);
        xFp32MasterAddr = (__ubuf__ float*)xFp32.GetPhyAddr() + int64_t(remainTile);
    }
    __local_mem__ float* workAddr = (__ubuf__ float*)workLocal.GetPhyAddr();
    __local_mem__ float* rstdAddr = (__ubuf__ float*)rstdLocal.GetPhyAddr();

    __VEC_SCOPE__
    {
        RegTensor<float> mainA, mainB, tailA, tailB, vMean, vDupReg, rstdReg;
        MaskReg pregMain = CreateMask<float, MaskPattern::ALL>();
        MaskReg pregMerge = CreateMask<float, MaskPattern::VL1>();
        MaskReg pregLoop;

        for (uint16_t i = 0; i < (uint16_t)remainRepeats; ++i) {
            pregLoop = UpdateMask<float>(remainSreg);
            LoadForHandleRemainV1(
                mainAddr, tailAddr, (i * 2 + 0) * V_LENGTH, (i * 2 + 1) * V_LENGTH, mainA, mainB, tailA, tailB,
                pregLoop, xFp32MainAddr, xFp32TailAddr);
            Add(mainA, mainA, tailA, pregLoop);
            Add(mainB, mainB, tailB, pregLoop);
            Add(mainA, mainA, mainB, pregLoop);
            ReduceSum(vMean, mainA, pregLoop);
            DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(workAddr + i, vMean, pregMerge);
        }
        for (uint16_t i = 0; i < (uint16_t)masterRepeats; ++i) {
            pregLoop = UpdateMask<float>(masterSreg);
            LoadForHandleMasterV1(
                masterAddr, (i * 2 + 0) * V_LENGTH, (i * 2 + 1) * V_LENGTH, mainA, mainB, pregLoop, xFp32MasterAddr);
            Add(mainA, mainA, mainB, pregLoop);
            ReduceSum(vMean, mainA, pregLoop);
            DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(workAddr + remainRepeats + i, vMean, pregMerge);
        }
        LocalMemBar<MemType::VEC_STORE, MemType::VEC_LOAD>();
        for (uint16_t i = 0; i < (uint16_t)mergeRepeats; ++i) {
            pregLoop = UpdateMask<float>(mergeSreg);
            DataCopy(mainA, workAddr + (i * 2 + 0) * V_LENGTH);
            DataCopy(mainB, workAddr + (i * 2 + 1) * V_LENGTH);
            Add(mainA, mainA, mainB, pregLoop);
            ReduceSum(vMean, mainA, pregLoop);
            DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(workAddr + i, vMean, pregMerge);
        }
        LocalMemBar<MemType::VEC_STORE, MemType::VEC_LOAD>();
        {
            pregLoop = UpdateMask<float>(meanSreg);
            DataCopy(mainA, workAddr + 0);
            ReduceSum(vMean, mainA, pregLoop);
            Muls(vMean, vMean, avgFactor, pregMerge);
            Adds(vMean, vMean, epsilon, pregMerge);
            Sqrt(vMean, vMean, pregMerge);
            Duplicate(vDupReg, float(1.0), pregMerge);
            Div(rstdReg, vDupReg, vMean, pregMerge);
            DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(rstdAddr + offset, rstdReg, pregMerge);
        }
    }
}

template <typename T>
__aicore__ inline void ComputeFormerImplV1MultiN(
    LocalTensor<T>& xLocal, LocalTensor<float>& xFp32, LocalTensor<float>& workLocal, LocalTensor<float>& rstdLocal,
    float avgFactor, float epsilon, uint32_t offset, uint32_t count, uint32_t powerSplit, uint32_t curRows)
{
    uint32_t remainTile = count - powerSplit;
    uint16_t remainRepeats = remainTile / (2 * V_LENGTH);

    uint32_t masterTile = powerSplit - remainTile;
    uint16_t masterRepeats = masterTile / (2 * V_LENGTH);

    uint32_t mergeTile = powerSplit / (2 * V_LENGTH);
    uint16_t mergeRepeats = mergeTile / (2 * V_LENGTH);

    uint32_t meanTile = mergeRepeats == 0 ? mergeTile : mergeRepeats;

    __local_mem__ T* mainAddr = (__ubuf__ T*)xLocal.GetPhyAddr();
    __local_mem__ T* tailAddr = (__ubuf__ T*)xLocal.GetPhyAddr() + int64_t(powerSplit);
    __local_mem__ T* masterAddr = (__ubuf__ T*)xLocal.GetPhyAddr() + int64_t(remainTile);
    __local_mem__ float *xFp32MainAddr, *xFp32TailAddr, *xFp32MasterAddr;
    if constexpr (is_same<T, half>::value || is_same<T, bfloat16_t>::value) {
        xFp32MainAddr = (__ubuf__ float*)xFp32.GetPhyAddr();
        xFp32TailAddr = (__ubuf__ float*)xFp32.GetPhyAddr() + int64_t(powerSplit);
        xFp32MasterAddr = (__ubuf__ float*)xFp32.GetPhyAddr() + int64_t(remainTile);
    }
    uint32_t curRowsAlign = CeilDiv((int32_t)curRows, 2);
    int64_t unrollOffset = (curRows / 2) * count;
    bool isWithTail = curRowsAlign - (curRows / 2);
    uint32_t tailOffset = offset + curRows / 2;

    __local_mem__ T* mainAddr1 = (__ubuf__ T*)xLocal.GetPhyAddr() + unrollOffset;
    __local_mem__ T* tailAddr1 = (__ubuf__ T*)xLocal.GetPhyAddr() + int64_t(powerSplit) + unrollOffset;
    __local_mem__ T* masterAddr1 = (__ubuf__ T*)xLocal.GetPhyAddr() + int64_t(remainTile) + unrollOffset;
    __local_mem__ float *xFp32MainAddr1, *xFp32TailAddr1, *xFp32MasterAddr1;
    if constexpr (is_same<T, half>::value || is_same<T, bfloat16_t>::value) {
        xFp32MainAddr1 = (__ubuf__ float*)xFp32.GetPhyAddr() + unrollOffset;
        xFp32TailAddr1 = (__ubuf__ float*)xFp32.GetPhyAddr() + int64_t(powerSplit) + unrollOffset;
        xFp32MasterAddr1 = (__ubuf__ float*)xFp32.GetPhyAddr() + int64_t(remainTile) + unrollOffset;
    }

    __local_mem__ float* workAddr = (__ubuf__ float*)workLocal.GetPhyAddr();
    __local_mem__ float* rstdAddr = (__ubuf__ float*)rstdLocal.GetPhyAddr();

    __local_mem__ float* workAddr1 =
        (__ubuf__ float*)workLocal.GetPhyAddr() + NormCommon::ONCE_VECTOR_SIZE * sizeof(float);
    __local_mem__ float* rstdAddr1 = (__ubuf__ float*)rstdLocal.GetPhyAddr() + curRows / 2;

    __VEC_SCOPE__
    {
        for (uint16_t row = 0; row < static_cast<uint16_t>(curRows / 2); row++) {
            uint32_t remainSreg = remainTile;
            uint32_t masterSreg = masterTile;
            uint32_t mergeSreg = mergeTile;
            uint32_t meanSreg = meanTile;
            RegTensor<float> mainA, mainB, tailA, tailB, vMean, vDupReg, rstdReg;
            MaskReg pregMain = CreateMask<float, MaskPattern::ALL>();
            MaskReg pregMerge = CreateMask<float, MaskPattern::VL1>();
            MaskReg pregLoop;

            uint32_t remainSreg1 = remainTile;
            uint32_t masterSreg1 = masterTile;
            uint32_t mergeSreg1 = mergeTile;
            uint32_t meanSreg1 = meanTile;
            RegTensor<float> mainA1, mainB1, tailA1, tailB1, vMean1, vDupReg1, rstdReg1;
            MaskReg pregMain1 = CreateMask<float, MaskPattern::ALL>();
            MaskReg pregMerge1 = CreateMask<float, MaskPattern::VL1>();
            MaskReg pregLoop1;

            for (uint16_t i = 0; i < (uint16_t)remainRepeats; ++i) {
                pregLoop = UpdateMask<float>(remainSreg);
                LoadForHandleRemainV1(
                    mainAddr, tailAddr, (i * 2 + 0) * V_LENGTH, (i * 2 + 1) * V_LENGTH, mainA, mainB, tailA, tailB,
                    pregLoop, xFp32MainAddr, xFp32TailAddr);
                Add(mainA, mainA, tailA, pregLoop);
                Add(mainB, mainB, tailB, pregLoop);
                Add(mainA, mainA, mainB, pregLoop);
                ReduceSum(vMean, mainA, pregLoop);
                DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(workAddr + i, vMean, pregMerge);
            }
            for (uint16_t i = 0; i < (uint16_t)masterRepeats; ++i) {
                pregLoop = UpdateMask<float>(masterSreg);
                LoadForHandleMasterV1(
                    masterAddr, (i * 2 + 0) * V_LENGTH, (i * 2 + 1) * V_LENGTH, mainA, mainB, pregLoop,
                    xFp32MasterAddr);
                Add(mainA, mainA, mainB, pregLoop);
                ReduceSum(vMean, mainA, pregLoop);
                DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(workAddr + remainRepeats + i, vMean, pregMerge);
            }
            // unroll part
            for (uint16_t i = 0; i < (uint16_t)remainRepeats; ++i) {
                pregLoop1 = UpdateMask<float>(remainSreg1);
                LoadForHandleRemainV1(
                    mainAddr1, tailAddr1, (i * 2 + 0) * V_LENGTH, (i * 2 + 1) * V_LENGTH, mainA1, mainB1, tailA1,
                    tailB1, pregLoop1, xFp32MainAddr1, xFp32TailAddr1);
                Add(mainA1, mainA1, tailA1, pregLoop1);
                Add(mainB1, mainB1, tailB1, pregLoop1);
                Add(mainA1, mainA1, mainB1, pregLoop1);
                ReduceSum(vMean1, mainA1, pregLoop1);
                DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(workAddr1 + i, vMean1, pregMerge1);
            }
            for (uint16_t i = 0; i < (uint16_t)masterRepeats; ++i) {
                pregLoop1 = UpdateMask<float>(masterSreg1);
                LoadForHandleMasterV1(
                    masterAddr1, (i * 2 + 0) * V_LENGTH, (i * 2 + 1) * V_LENGTH, mainA1, mainB1, pregLoop1,
                    xFp32MasterAddr1);
                Add(mainA1, mainA1, mainB1, pregLoop1);
                ReduceSum(vMean1, mainA1, pregLoop1);
                DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(workAddr1 + remainRepeats + i, vMean1, pregMerge1);
            }
            LocalMemBar<MemType::VEC_STORE, MemType::VEC_LOAD>();
            for (uint16_t i = 0; i < (uint16_t)mergeRepeats; ++i) {
                pregLoop = UpdateMask<float>(mergeSreg);
                DataCopy(mainA, workAddr + (i * 2 + 0) * V_LENGTH);
                DataCopy(mainB, workAddr + (i * 2 + 1) * V_LENGTH);
                Add(mainA, mainA, mainB, pregLoop);
                ReduceSum(vMean, mainA, pregLoop);
                DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(workAddr + i, vMean, pregMerge);
            }
            // unroll part
            for (uint16_t i = 0; i < (uint16_t)mergeRepeats; ++i) {
                pregLoop1 = UpdateMask<float>(mergeSreg1);
                DataCopy(mainA1, workAddr1 + (i * 2 + 0) * V_LENGTH);
                DataCopy(mainB1, workAddr1 + (i * 2 + 1) * V_LENGTH);
                Add(mainA1, mainA1, mainB1, pregLoop1);
                ReduceSum(vMean1, mainA1, pregLoop1);
                DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(workAddr1 + i, vMean1, pregMerge1);
            }
            LocalMemBar<MemType::VEC_STORE, MemType::VEC_LOAD>();
            {
                pregLoop = UpdateMask<float>(meanSreg);
                DataCopy(mainA, workAddr + 0);
                ReduceSum(vMean, mainA, pregLoop);
                Muls(vMean, vMean, avgFactor, pregMerge);
                Adds(vMean, vMean, epsilon, pregMerge);
                Sqrt(vMean, vMean, pregMerge);
                Duplicate(vDupReg, float(1.0), pregMerge);
                Div(rstdReg, vDupReg, vMean, pregMerge);
                DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(rstdAddr + offset, rstdReg, pregMerge);
            }
            // unroll part
            {
                pregLoop1 = UpdateMask<float>(meanSreg1);
                DataCopy(mainA1, workAddr1 + 0);
                ReduceSum(vMean1, mainA1, pregLoop1);
                Muls(vMean1, vMean1, avgFactor, pregMerge1);
                Adds(vMean1, vMean1, epsilon, pregMerge1);
                Sqrt(vMean1, vMean1, pregMerge1);
                Duplicate(vDupReg1, float(1.0), pregMerge1);
                Div(rstdReg1, vDupReg1, vMean1, pregMerge1);
                DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(rstdAddr1 + offset, rstdReg1, pregMerge1);
            }
            offset += 1;
            mainAddr += int64_t(count);
            tailAddr += int64_t(count);
            masterAddr += int64_t(count);
            xFp32MainAddr += int64_t(count);
            xFp32TailAddr += int64_t(count);
            xFp32MasterAddr += int64_t(count);

            mainAddr1 += int64_t(count);
            tailAddr1 += int64_t(count);
            masterAddr1 += int64_t(count);
            xFp32MainAddr1 += int64_t(count);
            xFp32TailAddr1 += int64_t(count);
            xFp32MasterAddr1 += int64_t(count);
        }
    }
    uint32_t tailDataOffset = unrollOffset + (curRows / 2) * count;
    __local_mem__ T* mainAddr2 = (__ubuf__ T*)xLocal.GetPhyAddr() + tailDataOffset;
    __local_mem__ T* tailAddr2 = (__ubuf__ T*)xLocal.GetPhyAddr() + int64_t(powerSplit) + tailDataOffset;
    __local_mem__ T* masterAddr2 = (__ubuf__ T*)xLocal.GetPhyAddr() + int64_t(remainTile) + tailDataOffset;
    __local_mem__ float *xFp32MainAddr2, *xFp32TailAddr2, *xFp32MasterAddr2;
    if constexpr (is_same<T, half>::value || is_same<T, bfloat16_t>::value) {
        xFp32MainAddr2 = (__ubuf__ float*)xFp32.GetPhyAddr() + tailDataOffset;
        xFp32TailAddr2 = (__ubuf__ float*)xFp32.GetPhyAddr() + int64_t(powerSplit) + tailDataOffset;
        xFp32MasterAddr2 = (__ubuf__ float*)xFp32.GetPhyAddr() + int64_t(remainTile) + tailDataOffset;
    }
    if (isWithTail) {
        __VEC_SCOPE__
        {
            uint32_t remainSreg1 = remainTile;
            uint32_t masterSreg1 = masterTile;
            uint32_t mergeSreg1 = mergeTile;
            uint32_t meanSreg1 = meanTile;
            RegTensor<float> mainA1, mainB1, tailA1, tailB1, vMean1, vDupReg1, rstdReg1;
            MaskReg pregMain1 = CreateMask<float, MaskPattern::ALL>();
            MaskReg pregMerge1 = CreateMask<float, MaskPattern::VL1>();
            MaskReg pregLoop1;

            for (uint16_t i = 0; i < (uint16_t)remainRepeats; ++i) {
                pregLoop1 = UpdateMask<float>(remainSreg1);
                LoadForHandleRemainV1(
                    mainAddr2, tailAddr2, (i * 2 + 0) * V_LENGTH, (i * 2 + 1) * V_LENGTH, mainA1, mainB1, tailA1,
                    tailB1, pregLoop1, xFp32MainAddr2, xFp32TailAddr2);
                Add(mainA1, mainA1, tailA1, pregLoop1);
                Add(mainB1, mainB1, tailB1, pregLoop1);
                Add(mainA1, mainA1, mainB1, pregLoop1);
                ReduceSum(vMean1, mainA1, pregLoop1);
                DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(workAddr1 + i, vMean1, pregMerge1);
            }
            for (uint16_t i = 0; i < (uint16_t)masterRepeats; ++i) {
                pregLoop1 = UpdateMask<float>(masterSreg1);
                LoadForHandleMasterV1(
                    masterAddr2, (i * 2 + 0) * V_LENGTH, (i * 2 + 1) * V_LENGTH, mainA1, mainB1, pregLoop1,
                    xFp32MasterAddr2);
                Add(mainA1, mainA1, mainB1, pregLoop1);
                ReduceSum(vMean1, mainA1, pregLoop1);
                DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(workAddr1 + remainRepeats + i, vMean1, pregMerge1);
            }
            LocalMemBar<MemType::VEC_STORE, MemType::VEC_LOAD>();
            for (uint16_t i = 0; i < (uint16_t)mergeRepeats; ++i) {
                pregLoop1 = UpdateMask<float>(mergeSreg1);
                DataCopy(mainA1, workAddr1 + (i * 2 + 0) * V_LENGTH);
                DataCopy(mainB1, workAddr1 + (i * 2 + 1) * V_LENGTH);
                Add(mainA1, mainA1, mainB1, pregLoop1);
                ReduceSum(vMean1, mainA1, pregLoop1);
                DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(workAddr1 + i, vMean1, pregMerge1);
            }
            LocalMemBar<MemType::VEC_STORE, MemType::VEC_LOAD>();
            {
                pregLoop1 = UpdateMask<float>(meanSreg1);
                DataCopy(mainA1, workAddr1 + 0);
                ReduceSum(vMean1, mainA1, pregLoop1);
                Muls(vMean1, vMean1, avgFactor, pregMerge1);
                Adds(vMean1, vMean1, epsilon, pregMerge1);
                Sqrt(vMean1, vMean1, pregMerge1);
                Duplicate(vDupReg1, float(1.0), pregMerge1);
                Div(rstdReg1, vDupReg1, vMean1, pregMerge1);
                DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(rstdAddr1 + tailOffset, rstdReg1, pregMerge1);
            }
        }
    }
}

template <typename T>
__aicore__ inline void LoadForHandleRemainV2(
    __local_mem__ T* mainAddr, __local_mem__ T* tailAddr, uint16_t offset1, uint16_t offset2, RegTensor<float>& mainA,
    RegTensor<float>& mainB, RegTensor<float>& tailA, RegTensor<float>& tailB, MaskReg& pregLoop)
{
    if constexpr (IsSameType<T, half>::value) {
        RegTensor<half> xFp16MainA, xFp16MainB, xFp16TailA, xFp16TailB;
        DataCopy<half, LoadDist::DIST_UNPACK_B16>(xFp16MainA, mainAddr + offset1);
        DataCopy<half, LoadDist::DIST_UNPACK_B16>(xFp16MainB, mainAddr + offset2);
        DataCopy<half, LoadDist::DIST_UNPACK_B16>(xFp16TailA, tailAddr + offset1);
        DataCopy<half, LoadDist::DIST_UNPACK_B16>(xFp16TailB, tailAddr + offset2);
        Cast<float, half, castTraitB162B32>(mainA, xFp16MainA, pregLoop);
        Cast<float, half, castTraitB162B32>(mainB, xFp16MainB, pregLoop);
        Cast<float, half, castTraitB162B32>(tailA, xFp16TailA, pregLoop);
        Cast<float, half, castTraitB162B32>(tailB, xFp16TailB, pregLoop);
        Mul(mainA, mainA, mainA, pregLoop);
        Mul(mainB, mainB, mainB, pregLoop);
        Mul(tailA, tailA, tailA, pregLoop);
        Mul(tailB, tailB, tailB, pregLoop);
    } else if constexpr (IsSameType<T, bfloat16_t>::value) {
        RegTensor<bfloat16_t> xBFp16MainA, xBFp16MainB, xBFp16TailA, xBFp16TailB;
        DataCopy<bfloat16_t, LoadDist::DIST_UNPACK_B16>(xBFp16MainA, mainAddr + offset1);
        DataCopy<bfloat16_t, LoadDist::DIST_UNPACK_B16>(xBFp16MainB, mainAddr + offset2);
        DataCopy<bfloat16_t, LoadDist::DIST_UNPACK_B16>(xBFp16TailA, tailAddr + offset1);
        DataCopy<bfloat16_t, LoadDist::DIST_UNPACK_B16>(xBFp16TailB, tailAddr + offset2);
        Cast<float, bfloat16_t, castTraitB162B32>(mainA, xBFp16MainA, pregLoop);
        Cast<float, bfloat16_t, castTraitB162B32>(mainB, xBFp16MainB, pregLoop);
        Cast<float, bfloat16_t, castTraitB162B32>(tailA, xBFp16TailA, pregLoop);
        Cast<float, bfloat16_t, castTraitB162B32>(tailB, xBFp16TailB, pregLoop);
        Mul(mainA, mainA, mainA, pregLoop);
        Mul(mainB, mainB, mainB, pregLoop);
        Mul(tailA, tailA, tailA, pregLoop);
        Mul(tailB, tailB, tailB, pregLoop);
    } else {
        DataCopy(mainA, mainAddr + offset1);
        DataCopy(mainB, mainAddr + offset2);
        DataCopy(tailA, tailAddr + offset1);
        DataCopy(tailB, tailAddr + offset2);
        Mul(mainA, mainA, mainA, pregLoop);
        Mul(mainB, mainB, mainB, pregLoop);
        Mul(tailA, tailA, tailA, pregLoop);
        Mul(tailB, tailB, tailB, pregLoop);
    }
}

template <typename T>
__aicore__ inline void LoadForHandleMasterV2(
    __local_mem__ T* masterAddr, uint16_t offset1, uint16_t offset2, RegTensor<float>& mainA, RegTensor<float>& mainB,
    MaskReg& pregLoop)
{
    if constexpr (IsSameType<T, half>::value) {
        RegTensor<half> xFp16MainA, xFp16MainB;
        DataCopy<half, LoadDist::DIST_UNPACK_B16>(xFp16MainA, masterAddr + offset1);
        DataCopy<half, LoadDist::DIST_UNPACK_B16>(xFp16MainB, masterAddr + offset2);
        Cast<float, half, castTraitB162B32>(mainA, xFp16MainA, pregLoop);
        Cast<float, half, castTraitB162B32>(mainB, xFp16MainB, pregLoop);
        Mul(mainA, mainA, mainA, pregLoop);
        Mul(mainB, mainB, mainB, pregLoop);
    } else if constexpr (IsSameType<T, bfloat16_t>::value) {
        RegTensor<bfloat16_t> xBFp16MainA, xBFp16MainB;
        DataCopy<bfloat16_t, LoadDist::DIST_UNPACK_B16>(xBFp16MainA, masterAddr + offset1);
        DataCopy<bfloat16_t, LoadDist::DIST_UNPACK_B16>(xBFp16MainB, masterAddr + offset2);
        Cast<float, bfloat16_t, castTraitB162B32>(mainA, xBFp16MainA, pregLoop);
        Cast<float, bfloat16_t, castTraitB162B32>(mainB, xBFp16MainB, pregLoop);
        Mul(mainA, mainA, mainA, pregLoop);
        Mul(mainB, mainB, mainB, pregLoop);
    } else {
        DataCopy(mainA, masterAddr + offset1);
        DataCopy(mainB, masterAddr + offset2);
        Mul(mainA, mainA, mainA, pregLoop);
        Mul(mainB, mainB, mainB, pregLoop);
    }
}

template <typename T>
__aicore__ inline void ComputeFormerImplV2(
    LocalTensor<float>& dstLocal, LocalTensor<T>& xLocal, LocalTensor<float>& workLocal, uint32_t offset,
    uint32_t count, uint32_t powerSplit)
{
    uint32_t remainTile = count - powerSplit;
    uint32_t remainSreg = remainTile;
    uint16_t remainRepeats = remainTile / (2 * V_LENGTH);

    uint32_t masterTile = powerSplit - remainTile;
    uint32_t masterSreg = masterTile;
    uint16_t masterRepeats = masterTile / (2 * V_LENGTH);

    uint32_t mergeTile = powerSplit / (2 * V_LENGTH);
    uint32_t mergeSreg = mergeTile;
    uint16_t mergeRepeats = mergeTile / (2 * V_LENGTH);

    uint32_t meanTile = mergeRepeats == 0 ? mergeTile : mergeRepeats;
    uint32_t meanSreg = meanTile;

    __local_mem__ T* mainAddr = (__ubuf__ T*)xLocal.GetPhyAddr();
    __local_mem__ T* tailAddr = (__ubuf__ T*)xLocal.GetPhyAddr() + int64_t(powerSplit);
    __local_mem__ T* masterAddr = (__ubuf__ T*)xLocal.GetPhyAddr() + int64_t(remainTile);

    __local_mem__ float* workAddr = (__ubuf__ float*)workLocal.GetPhyAddr();
    __local_mem__ float* dstAddr = (__ubuf__ float*)dstLocal.GetPhyAddr();

    __VEC_SCOPE__
    {
        RegTensor<float> mainA, mainB, tailA, tailB, vMean, vDupReg, rstdReg;
        MaskReg pregMain = CreateMask<float, MaskPattern::ALL>();
        MaskReg pregMerge = CreateMask<float, MaskPattern::VL1>();
        MaskReg pregLoop;

        for (uint16_t i = 0; i < (uint16_t)remainRepeats; ++i) {
            pregLoop = UpdateMask<float>(remainSreg);
            LoadForHandleRemainV2(
                mainAddr, tailAddr, (i * 2 + 0) * V_LENGTH, (i * 2 + 1) * V_LENGTH, mainA, mainB, tailA, tailB,
                pregLoop);
            Add(mainA, mainA, tailA, pregLoop);
            Add(mainB, mainB, tailB, pregLoop);
            Add(mainA, mainA, mainB, pregLoop);
            ReduceSum(vMean, mainA, pregLoop);
            DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(workAddr + i, vMean, pregMerge);
        }
        for (uint16_t i = 0; i < (uint16_t)masterRepeats; ++i) {
            pregLoop = UpdateMask<float>(masterSreg);
            LoadForHandleMasterV2(masterAddr, (i * 2 + 0) * V_LENGTH, (i * 2 + 1) * V_LENGTH, mainA, mainB, pregLoop);
            Add(mainA, mainA, mainB, pregLoop);
            ReduceSum(vMean, mainA, pregLoop);
            DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(workAddr + remainRepeats + i, vMean, pregMerge);
        }
        LocalMemBar<MemType::VEC_STORE, MemType::VEC_LOAD>();
        for (uint16_t i = 0; i < (uint16_t)mergeRepeats; ++i) {
            pregLoop = UpdateMask<float>(mergeSreg);
            DataCopy(mainA, workAddr + (i * 2 + 0) * V_LENGTH);
            DataCopy(mainB, workAddr + (i * 2 + 1) * V_LENGTH);
            Add(mainA, mainA, mainB, pregLoop);
            ReduceSum(vMean, mainA, pregLoop);
            DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(workAddr + i, vMean, pregMerge);
        }
        LocalMemBar<MemType::VEC_STORE, MemType::VEC_LOAD>();
        {
            pregLoop = UpdateMask<float>(meanSreg);
            DataCopy(mainA, workAddr + 0);
            ReduceSum(vMean, mainA, pregLoop);
            DataCopy<float, StoreDist::DIST_FIRST_ELEMENT_B32>(dstAddr + offset, vMean, pregMerge);
        }
    }
}

} // namespace RmsNorm
#endif // OPS_BUILT_IN_TBE_IMPL_ASCENDC_RMS_NORM_REGBASE_COMMON_H